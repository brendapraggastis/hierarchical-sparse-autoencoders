{"model_id": "gpt2-small", "layer": 0, "sae_set": "res-jb", "features": [{"feature_index": 2, "neuron_alignment_indices": [88, 171, 27], "neuron_alignment_values": [0.109, 0.107, 0.103], "neuron_alignment_l1": [0.005, 0.005, 0.005], "correlated_neurons_indices": [480, 266, 87], "correlated_neurons_l1": [0.014, 0.014, 0.014], "correlated_neurons_pearson": [0.024, 0.023, 0.023], "correlated_features_indices": [2, 3], "correlated_features_l1": [0.0, 0.0], "correlated_features_pearson": [0.0, -0.0], "neg_str": ["\u00e3\u0124\u00b1", "ivil", " evasion", "chie", " pursu", " diversion", "pled", " Dod", "lement", "anna"], "neg_values": [-0.743, -0.724, -0.716, -0.711, -0.709, -0.705, -0.697, -0.692, -0.685, -0.681], "pos_str": [" MRI", "MRI", " ultrasound", " autopsy", " microscope", " microsc", "opsy", " anatomical", " imaging", " Ultr"], "pos_values": [3.121, 2.635, 2.313, 2.124, 1.738, 1.627, 1.539, 1.464, 1.42, 1.398], "frac_nonzero": 0.00035999999999999997, "freq_hist_data_bar_values": [0.008, 0.024, 0.04, 0.056, 0.072, 0.088, 0.104, 0.12, 0.136, 0.152, 0.168, 0.184, 0.2, 0.216, 0.232, 0.248, 0.264, 0.28, 0.296, 0.312, 0.328, 0.344, 0.36, 0.376, 0.392, 0.408, 0.424, 0.44, 0.456, 0.472, 0.488, 0.504, 0.52, 0.536, 0.552, 0.568, 0.584, 0.6, 0.616, 0.632, 0.648, 0.664, 0.68, 0.696, 0.712, 0.728, 0.744, 0.76, 0.776, 0.792], "freq_hist_data_bar_heights": [14, 11, 13, 26, 29, 27, 11, 15, 8, 7, 10, 3, 5, 8, 3, 7, 2, 7, 1, 5, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3], "logits_hist_data_bar_heights": [11, 30, 120, 341, 953, 2029, 3880, 5924, 7803, 8318, 7439, 5341, 3412, 2072, 1145, 615, 366, 176, 112, 59, 41, 25, 15, 7, 4, 5, 2, 4, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1], "logits_hist_data_bar_values": [-0.704, -0.627, -0.55, -0.473, -0.395, -0.318, -0.241, -0.163, -0.086, -0.009, 0.068, 0.146, 0.223, 0.3, 0.378, 0.455, 0.532, 0.609, 0.687, 0.764, 0.841, 0.919, 0.996, 1.073, 1.15, 1.228, 1.305, 1.382, 1.46, 1.537, 1.614, 1.691, 1.769, 1.846, 1.923, 2.001, 2.078, 2.155, 2.232, 2.31, 2.387, 2.464, 2.542, 2.619, 2.696, 2.773, 2.851, 2.928, 3.005, 3.083], "n_prompts_total": 5000, "n_tokens_in_prompt": 128, "dataset": "Skylion007/openwebtext", "decoder_weights_dist": [], "activations": [{"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" would", " nonetheless", " suggest", " for", " complexity", " relevant", " to", " those", " types", ",", " the", " general", " case", " being", " made", " against", " conservatives", " in", " those", " areas", " needs", " revision", ".", " This", " would", " present", " to", " us", " a", " more", " nuanced", " and", " accurate", " picture", " of", " the", " relationship", " between", " ideology", " and", " complexity", " and", " suggest", " at", " the", " least", " that", " the", " current", " picture", " must", " be", " qualified", " by", " the", " type", " of", " complexity", " measurement", " under", " the", " microscope", ".", " The", " potential", " pitfalls", " of", " self", "\u00e2\u0122\u0132", "report", " measurement", " Further", ",", " for", " the", " purposes", " of", " determining", " the", " average", " complexity", " of", " a", " group", " of", " persons", ",", " the", " kinds", " of", " self", "\u00e2\u0122\u0132", "report", " measurements", " comprising", " the", " bulk", " of", " prior", " meta", "\u00e2\u0122\u0132", "an", "alyses", " have", " some", " additional", " potential", " pitfalls", " in", " interpretation", ".", " A", " self", "\u00e2\u0122\u0132", "report", " measurement", " is", " at", " best", " an", " indirect", " marker", " of", " a", " potential", " complexity", "\u00e2\u0122\u0132"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 61, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" likely", " dict", "ating", " under", " the", " gun", ".", "\n", "\n", "And", ",", " lo", " and", " behold", ",", " on", " the", " list", " of", " ad", "her", "ing", " parties", ",", " A", "IG", " takes", " front", " and", " center", " stage", " (", "together", " with", " several", " other", " parties", " that", " probably", " deserve", " the", " microscope", " treatment", ").", "\n", "\n", "So", " -", " in", " simple", " terms", ",", " IS", "DA", ",", " which", " is", " the", " only", " effective", " supervisor", " of", " the", " Over", " The", " Counter", " C", "DS", " market", ",", " is", " giving", " its", " blessing", " for", " trades", " to", " occur", " (", "cross", ")", " below", " where", " there", " is", " a", " realistic", " market", " bid", ",", " or", " higher", " than", " the", " offer", ".", " In", " traditional", " equity", " markets", " this", " is", " a", " highly", " illegal", " practice", ".", " IS", "DA", " is", " allowing", " retrospective", " arbitrary", " trades", " to", " have", " occurred", " at", " whatever", " price", " any", " two", " parties", " agree", " on", ",", " so"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.797, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 41, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["\u2014", "t", "otally", ",", " completely", " flat", "?", "\n", "\n", "R", "is", "hel", " is", " very", " familiar", " with", " the", " Kansas", " flat", "ness", " question", ".", " He", " was", " a", " student", " of", " Mark", " F", "on", "stad", ",", " a", " Texas", " State", " ge", "ographer", " who", ",", " in", " 2003", ",", " set", " out", " with", " some", " colleagues", " and", " a", " laser", " microscope", " to", " determine", " which", " was", " fl", "atter", ":", " Kansas", " or", " an", " I", "H", "OP", " panc", "ake", ".", " The", " resulting", " study", ",", " titled", " \u00e2\u0122", "\u013e", "Kansas", " Is", " Fl", "atter", " Than", " a", " Panc", "ake", ",", "\u00e2\u0122", "\u013f", " likely", " added", " to", " the", " public", " misconceptions", " that", " rank", "le", " Dob", "son", " and", " Campbell", ".", " (", "They", " also", " point", " out", " that", ",", " if", " you", " use", " the", " particular", " mathematical", " approach", " of", " F", "on", "stad", " et", " al", ",", " \u00e2\u0122", "\u013e", "there", " is", " no", " place", " on"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["%", " of", " their", " genome", " inherited", " from", " European", " ancestors", ".[", "173", "]", " If", " racial", " IQ", " gaps", " have", " a", " partially", " genetic", " basis", ",", " one", " might", " expect", " blacks", " with", " a", " higher", " degree", " of", " European", " ancestry", " to", " score", " higher", " on", " IQ", " tests", " than", " blacks", " with", " less", " European", " ancestry", ",", " because", " the", " genes", " inherited", " from", " European", " ancestors", " would", " likely", " include", " some", " genes", " with", " a", " positive", " effect", " on", " IQ", ".", " Genetic", "ist", " Alan", " Temple", "ton", " has", " argued", " that", " an", " experiment", " based", " on", " the", " Mend", "el", "ian", " \"", "common", " garden", "\"", " design", " where", " specimens", " with", " different", " hybrid", " compositions", " are", " subjected", " to", " the", " same", " environmental", " influences", ",", " would", " be", " the", " only", " way", " to", " definitively", " show", " a", " causal", " relation", " between", " genes", " and", " IQ", ".", " Sum", "mar", "izing", " the", " findings", " of", " adm", "ixture", " studies", ",", " he", " concludes"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 86, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" guidance", " and", " navigation", " systems", ".", " (", "Dr", ".", " Kh", "alf", "in", "\u00e2\u0122", "\u013b", "s", " bio", " is", " below", " the", " video", ".)", "\n", "\n", "I", "gor", " Kh", "alf", "in", ",", " PhD", "\n", "\n", "Dr", ".", " Kh", "alf", "in", " has", " over", " 30", " years", " of", " experience", " in", " the", " aerospace", "/", "defense", " industries", " and", " academia", ",", " holds", " 5", " patents", " in", " electromagnetic", " tracking", " and", " has", " published", " over", " 40", " scientific", " papers", " on", " related", " topics", ".", " His", " expertise", " includes", " opt", "oe", "lect", "ronics", ",", " electrom", "ag", "net", "ics", ",", " imaging", " systems", " and", " sensors", ",", " remote", " sensing", ",", " applied", " physics", ",", " signal", " processing", ",", " and", " non", "linear", " systems", ".", " Dr", ".", " Kh", "alf", "in", " is", " currently", " employed", " with", " Lockheed", " Martin", " Space", " Systems", " Company", ",", " where", " he", " leads", " the", " projects", " on", " spacecraft", " control", " systems", ".", " His", " prior"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 81, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["Man", "l", "ius", "\n", "\n", "Editor", "ial", " failed", " to", " address", " privacy", " and", " civil", " liberties", " issues", "\n", "\n", "To", " the", " Editor", ":", "\n", "\n", "It", " is", " readily", " understandable", " that", " news", " corporations", " would", " want", " to", " use", " drones", " to", " record", " and", " report", " news", ".", " The", " imaging", " capabilities", " of", " drones", " are", " mighty", " impressive", ".", "\n", "\n", "Yes", ",", " drones", " might", " \"", "enh", "ance", " the", " public", "'s", " understanding", " of", " events", ".\"", " And", ",", " true", ",", " drones", " may", " well", " have", " been", " \"", "used", " to", " capture", " footage", " of", " massive", " demonstrations", " in", " Ukraine", ".\"", " (", "Indeed", ",", " drones", " are", " ideal", " for", " identifying", " and", " monitoring", " demonstrators", ".)", "\n", "\n", "However", ",", " your", " May", " 11", " editorial", ",", " \"", "Journal", "ists", " have", " right", " to", " use", " drones", ",", " too", ",\"", " under", " analyzed", " the", " issue", ".", " Sure", ",", " news", " corporations"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 42, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["\u013e", "V", "irt", "ually", " every", " time", " Harvey", " was", " hired", " for", " a", " television", " show", ",", " [", "Co", "oper", "]", " would", " contact", " the", " owners", " or", " principals", " to", " inform", " them", " of", " potentially", " embarrassing", " material", " and", "/", "or", " tapes", " and", " attempt", " to", " have", " them", " influence", " Harvey", " to", " pay", " for", " the", " tapes", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Interestingly", ",", " court", " documents", " appear", " to", " show", " that", " Harvey", " admits", " to", " the", " r", "ants", ",", " saying", " that", " at", " times", " he", " was", " ed", "g", "ier", " than", " others", ".", " \u00e2\u0122", "\u013e", "I", " didn", "\u00e2\u0122", "\u013b", "t", " have", " to", " concern", " myself", " with", " branding", " or", " imaging", " or", " anything", ".", " You", " could", " just", " say", " \u2014", " I", " thought", " I", " was", " funn", "ier", ",", "\u00e2\u0122", "\u013f", " Harvey", " said", ".", "\n", "\n", "Co", "oper", " alleges", " that", " on", " one", " tape", ",", " Harvey", " says", " it"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 93, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" darker", " aspect", " of", " this", " kind", " of", " cultural", " pathology", ",", " just", " as", " there", " are", " serious", " analyses", " pointing", " to", " the", " socially", " toxic", " effects", " of", " the", " JFK", " assassination", " \u00e2\u0122", "\u013e", "altern", "ate", " theories", "\u00e2\u0122", "\u013f", ".", " For", " space", "flight", ",", " being", " distracted", " by", " the", " wrong", " cause", " means", " being", " tempted", " by", " the", " wrong", " fix", ".", " That", "\u00e2\u0122", "\u013b", "s", " never", " amusing", ",", " and", " often", " can", " be", " expensive", ".", " For", " space", "flight", ",", " being", " distracted", " by", " the", " wrong", " cause", " means", " being", " tempted", " by", " the", " wrong", " fix", ".", " That", "\u00e2\u0122", "\u013b", "s", " never", " amusing", ",", " and", " often", " can", " be", " expensive", ".", " As", " an", " egregious", " \u00e2\u0122", "\u013e", "bad", " example", "\u00e2\u0122", "\u013f", " of", " wrong", " causes", ",", " a", " recent", " book", " (", "Dark", " Mission", ",", " by", " Richard", " Ho", "ag", "land", " and", " Michael", " Bar", "a", ")", " spent", " a"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 7, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" shield", " (~", "$", "2", ".", " This", " slides", " over", " a", " fluorescent", " bulb", " to", " prevent", " it", " from", " shattering", ")", " Small", " piece", " of", " aluminum", " foil", ".", " (", "Had", " it", " in", " the", " house", ",", " because", " we", "\u00e2\u0122", "\u013b", "re", " civilized", ")", "\n", "\n", "The", " Steps", "\n", "\n", "I", " redesigned", " the", " PVC", " pipe", " and", " made", " it", " about", " 5", "\u00e2\u0122\u00b3", " shorter", ".", " It", " was", " too", " long", " and", " unw", "ield", "y", ".", " I", " took", " a", " little", " more", " care", " in", " marking", " off", " the", " space", " to", " cut", " out", ".", " The", " blue", " tape", " worked", " perfect", " to", " keep", " a", " straight", " line", ".", "\n", "\n", "Cut", " the", " opening", " in", " the", " PVC", " pipe", ".", " This", " can", " be", " whatever", " width", " you", " want", ".", " Mine", " is", " a", " little", " less", " than", " halfway", " through", ".", "\n", "\n", "Cut", " the", " fl", "ou", "rescent", " shield", " to"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 9, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" matte", " black", " spray", "p", "aint", " that", " was", " specific", " for", " plastics", ".", " It", " bonds", " well", " and", " only", " takes", " about", " 15", " minutes", " to", " dry", ".", " Several", " thin", " coats", " will", " work", " better", " than", " one", " thick", " coat", ".", " This", " is", " after", " a", " couple", " coats", " of", " paint", " (", "the", " paint", " shield", " keeps", " the", " inside", " clean", " and", " white", "):", "\n", "\n", "After", " its", " dried", ",", " its", " time", " to", " fit", " it", " onto", " the", " flashlight", ".", " Replace", " the", " painted", " fluorescent", " shield", " with", " the", " sand", "ed", " white", " shield", ".", "\n", "\n", "For", " the", " Cou", "pler", " I", " used", ",", " I", " had", " to", " cut", " a", " small", " (", "2", " cm", ")", " piece", " of", " PVC", " pipe", " to", " fill", " in", " the", " gap", " on", " the", " flashlight", " side", " of", " the", " cou", "pler", ".", " The", " clear", " plastic", " for", " focusing", " the", " flashlight", " falls", " out", " if"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.353, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 71, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" the", " same", " length", " as", " the", " pipe", ".", " Then", " use", " a", " utility", " knife", " to", " cut", " it", " long", "ways", ".", "\n", "\n", "Sand", " the", " inside", " and", " outside", " of", " the", " fluorescent", " shield", " to", " create", " a", " \u00e2\u0122", "\u013e", "sm", "oked", " glass", "\u00e2\u0122", "\u013f", " look", " and", " set", " aside", ".", " This", " shows", " how", " I", " got", " the", " fluorescent", " shield", " to", " stay", " large", " enough", " to", " snug", "ly", " fit", " into", " the", " pipe", ".", " Just", " a", " simple", " slit", " on", " one", " side", " and", " the", " other", " side", " slides", " in", ".", " Also", " you", " can", " see", " how", " the", " once", " clear", " plastic", " looks", " after", " sand", "ing", " it", ":", "\n", "\n", "Remove", " the", " \u00e2\u0122", "\u013e", "c", "rown", "\u00e2\u0122", "\u013f", " off", " of", " the", " bulb", " end", " of", " the", " flashlight", ".", " You", " will", " still", " have", " the", " reflect", "or", " cone", " and", " clear", " focus", "er", " inside", " the", " light"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" the", " same", " length", " as", " the", " pipe", ".", " Then", " use", " a", " utility", " knife", " to", " cut", " it", " long", "ways", ".", "\n", "\n", "Sand", " the", " inside", " and", " outside", " of", " the", " fluorescent", " shield", " to", " create", " a", " \u00e2\u0122", "\u013e", "sm", "oked", " glass", "\u00e2\u0122", "\u013f", " look", " and", " set", " aside", ".", " This", " shows", " how", " I", " got", " the", " fluorescent", " shield", " to", " stay", " large", " enough", " to", " snug", "ly", " fit", " into", " the", " pipe", ".", " Just", " a", " simple", " slit", " on", " one", " side", " and", " the", " other", " side", " slides", " in", ".", " Also", " you", " can", " see", " how", " the", " once", " clear", " plastic", " looks", " after", " sand", "ing", " it", ":", "\n", "\n", "Remove", " the", " \u00e2\u0122", "\u013e", "c", "rown", "\u00e2\u0122", "\u013f", " off", " of", " the", " bulb", " end", " of", " the", " flashlight", ".", " You", " will", " still", " have", " the", " reflect", "or", " cone", " and", " clear", " focus", "er", " inside", " the", " light"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 27, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["We", " take", " elements", " of", " the", " scans", " and", " then", " we", " add", " to", " them", ",\"", " explains", " Yu", ".", " \"", "When", " we", " want", " a", " particular", " shape", ",", " we", " add", " pose", "-", "based", " deform", "ers", ".", " We", " can", " sculpt", " the", " exact", " shape", " we", " want", " all", " over", " the", " body", ":", " on", " the", " knees", ",", " shoulders", ",", " elbows", ",", " the", " waist", ".", " \"", "In", " addition", " to", " those", " pose", "-", "based", " deform", "ers", ",", " we", " add", " wr", "inkle", " maps", " as", " well", ".", " When", " she", " bends", " a", " joint", ",", " we", " actually", " see", " the", " wrinkles", " in", " her", " shirt", " change", ".", " Or", " if", " she", " bends", " her", " arm", " you", "\u00e2\u0122", "\u013b", "ll", " see", " the", " mus", "cul", "ature", " in", " her", " shoulders", " deform", ".\"", " Yu", "\u00e2\u0122", "\u013b", "s", " background", " is", " in", " anatomical", " art", " and", " medical", " illustrations", ".", " Anat", "omical", " models"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 118, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" and", " images", " can", " be", " seen", " around", " the", " desks", " of", " the", " art", " department", " in", " Crystal", " Dynamics", "\u00e2\u0122", "\u013b", " offices", ".", " \"", "We", " could", " never", " get", " this", " level", " of", " anatomical", " correctness", " in", " the", " character", " before", ",\"", " says", " Game", " Director", " Brian", " Horton", ",", " referring", " to", " the", " consoles", " and", " the", " development", " technology", " now", " available", ".", " \"", "The", " systems", " just", " wouldn", "\u00e2\u0122", "\u013b", "t", " allow", " for", " it", ".", " Now", " we", " have", " so", " much", " more", " artist", " control", ".", " \"", "From", " a", " technology", " standpoint", ",", " we", "\u00e2\u0122", "\u013b", "ve", " put", " a", " lot", " of", " time", " and", " energy", " into", " the", " formation", " of", " Lara", "\u00e2\u0122", "\u013b", "s", " character", ".", " Not", " only", " her", " face", ",", " but", " her", " body", ".", " We", "\u00e2\u0122", "\u013b", "ve", " created", " a", " whole", " new", " animation", " system", " that", "\u00e2\u0122", "\u013b", "s", " based", " on", " artist", "-", "sc"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.33, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 27, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["),", " Devin", " T", "oner", " (", "L", "ans", "down", "e", "/", "Le", "in", "ster", "),", " Paul", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "Young", " Mun", "ster", ")", " (", "capt", "),", " Peter", " O", "\u00e2\u0122", "\u013b", "Mah", "ony", " (", "C", "ork", " Constitution", "/", "M", "un", "ster", "),", " Sean", " O", "\u00e2\u0122", "\u013b", "Brien", " (", "U", "CD", "/", "Le", "in", "ster", "),", " Jamie", " He", "as", "lip", " (", "Dub", "lin", " University", "/", "Le", "in", "ster", ").", " Repl", "acements", ":", " Ian", " Mad", "igan", " (", "Black", "rock", " College", "/", "Le", "in", "ster", ")", " for", " Se", "xton", " (", "28", " mins", "),", "\n", "\n", "I", "ain", " Henderson", " (", "B", "ally", "nah", "inch", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "half", "=", "time", "),", " Chris", " Henry", " (", "Mal", "one", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 120, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["\n", "\n", "Ireland", ":", "Rob", " Kear", "ney", " (", "U", "CD", "/", "Le", "in", "ster", ");", " Tommy", " Bow", "e", " (", "B", "elf", "ast", " Har", "le", "qu", "ins", "/", "Ul", "ster", "),", " Keith", " Ear", "ls", " (", "Young", " Mun", "ster", "/", "M", "un", "ster", "),", " Robbie", " H", "ens", "haw", " (", "B", "ucc", "aneers", "/", "Conn", "acht", "),", " Dave", " Kear", "ney", " (", "L", "ans", "down", "e", "/", "Le", "in", "ster", ");", " Jonathan", " Se", "xton", " (", "St", ".", " Mary", "\u00e2\u0122", "\u013b", "s", " College", "/", "Le", "in", "ster", "),", " Conor", " Murray", " (", "G", "arry", "ow", "en", "/", "M", "un", "ster", ");", " C", "ian", " He", "aly", " (", "Cl", "ont", "arf", "/", "Le", "in", "ster", "),", " Rory", " Best", " (", "Ban", "bridge", "/", "Ul", "ster", "),", " Mike", " Ross", " (", "Cl", "ont", "arf", "/", "Le", "in", "ster"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 114, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["),", " Devin", " T", "oner", " (", "L", "ans", "down", "e", "/", "Le", "in", "ster", "),", " Paul", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "Young", " Mun", "ster", ")", " (", "capt", "),", " Peter", " O", "\u00e2\u0122", "\u013b", "Mah", "ony", " (", "C", "ork", " Constitution", "/", "M", "un", "ster", "),", " Sean", " O", "\u00e2\u0122", "\u013b", "Brien", " (", "U", "CD", "/", "Le", "in", "ster", "),", " Jamie", " He", "as", "lip", " (", "Dub", "lin", " University", "/", "Le", "in", "ster", ").", " Repl", "acements", ":", " Ian", " Mad", "igan", " (", "Black", "rock", " College", "/", "Le", "in", "ster", ")", " for", " Se", "xton", " (", "28", " mins", "),", "\n", "\n", "I", "ain", " Henderson", " (", "B", "ally", "nah", "inch", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "half", "=", "time", "),", " Chris", " Henry", " (", "Mal", "one", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 101, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" an", "ubis", ".", "\n", "\n", "Second", "ary", " characters", " [", " edit", " ]", "\n", "\n", "Tam", "my", ":", " Tammy", " is", " a", " crazy", " witch", " therapist", " who", " constantly", " tries", " to", " sed", "uce", " Dr", ".", " Ul", "rick", ".", " She", " provides", " therapy", " to", " Pit", "uka", " and", " N", "elly", ".", "\n", "\n", "Dr", ".", "Ul", "rick", ":", " Ul", "rick", " is", " a", " doctor", " that", " works", " in", " P", "aws", " Den", " at", " a", " clinic", ".", " Because", " of", " his", " shy", " and", " cute", " personality", ",", " many", " of", " the", " females", " find", " him", " very", " attractive", ".", "\n", "\n", "Notes", " [", " edit", " ]", "\n", "\n", "\"", "Fill", "ers", "\"", " are", " used", " whenever", " a", " comic", " isn", "'t", " av", "a", "ible", "\n", "\n", "Up", "dates", " are", " sometimes", " streamed", "<|endoftext|>", "Four", " reasons", " the", " updated", " Uber", " Dubai", " app", " makes", " us", " sad", " By", " Mike", " Priest", "\n", "\n"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.318, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 47, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" Miami", " but", " my", " mind", " is", " in", " Haiti", ",\"", " Fletcher", " T", "ous", "s", "aint", ",", " a", " young", " immigrant", ",", " tells", " the", " BBC", ".", "\n", "\n", "13", "10", " Two", " days", " after", " the", " disaster", " struck", ",", " people", " speak", " of", " still", " hearing", " voices", " crying", " from", " the", " rubble", ".", " Two", " days", " after", " the", " disaster", " struck", ",", " people", " speak", " of", " still", " hearing", " voices", " crying", " from", " the", " rubble", ".", "\n", "\n", "130", "9", " Google", " and", " Ge", "oe", "ye", " have", " put", " together", " some", " Google", " and", " Ge", "oe", "ye", " have", " put", " together", " some", " new", " satellite", " images", " of", " Haiti", ",", " taken", " after", " the", " quake", " and", " showing", " the", " extent", " of", " the", " destruction", ".", "\n", "\n", "130", "3", " The", " foreign", " minister", " of", " Indonesia", " -", " a", " country", " which", " has", " suffered", " natural", " disasters", " in", " the", " past", " -", " expressed", " condolences", " to", " Haiti"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.314, 0.0, 0.0], "qualifying_token_index": 124, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["\n", "\n", "Ireland", ":", "Rob", " Kear", "ney", " (", "U", "CD", "/", "Le", "in", "ster", ");", " Tommy", " Bow", "e", " (", "B", "elf", "ast", " Har", "le", "qu", "ins", "/", "Ul", "ster", "),", " Keith", " Ear", "ls", " (", "Young", " Mun", "ster", "/", "M", "un", "ster", "),", " Robbie", " H", "ens", "haw", " (", "B", "ucc", "aneers", "/", "Conn", "acht", "),", " Dave", " Kear", "ney", " (", "L", "ans", "down", "e", "/", "Le", "in", "ster", ");", " Jonathan", " Se", "xton", " (", "St", ".", " Mary", "\u00e2\u0122", "\u013b", "s", " College", "/", "Le", "in", "ster", "),", " Conor", " Murray", " (", "G", "arry", "ow", "en", "/", "M", "un", "ster", ");", " C", "ian", " He", "aly", " (", "Cl", "ont", "arf", "/", "Le", "in", "ster", "),", " Rory", " Best", " (", "Ban", "bridge", "/", "Ul", "ster", "),", " Mike", " Ross", " (", "Cl", "ont", "arf", "/", "Le", "in", "ster"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 27, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" us", " being", " \u00e2\u0122", "\u013e", "tra", "ps", "\u00e2\u0122", "\u013f", " are", " a", " bit", " too", " numerous", " to", " name", " them", " all", ".", " A", " few", " that", " come", " to", " mind", " are", " the", " basic", " assumption", " that", " we", "\u00e2\u0122", "\u013b", "re", " \u00e2\u0122", "\u013e", "really", "\u00e2\u0122", "\u013f", " men", ",", " believing", " that", " our", " decisions", " all", " rev", "olve", " around", " you", " and", "we", "\u00e2\u0122", "\u013b", "re", " doing", " this", " for", " your", " sake", ",", " not", " our", " own", " (", "kind", " of", " like", " the", " earlier", " example", " about", " how", " men", " may", " interpret", " how", " a", " woman", " dresses", "),", " the", " issues", " of", " conf", "l", "ating", " gender", " expression", " with", " sexual", " motivations", ",", " the", " concept", " that", " fem", "al", "eness", " and", " femin", "inity", " are", " art", "ifice", " and", " fake", ",", " etc", ".", "\n", "\n", "But", " I", " guess", " the", " one", " that", " I", "\u00e2\u0122", "\u013b", "d", " most", " like", " to", " un", "pack", " is"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.308, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 95, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [".", " \"", "As", " a", " country", " that", " has", " been", " itself", " devastated", " by", " a", " similar", " situation", ",", " we", " are", " absolutely", " saddened", " by", " what", "'s", " happening", " in", " Haiti", ",\"", " said", " Marty", " Nat", "ale", "g", "awa", ".", " The", " foreign", " minister", " of", " Indonesia", " -", " a", " country", " which", " has", " suffered", " natural", " disasters", " in", " the", " past", " -", " expressed", " condolences", " to", " Haiti", ".", " \"", "As", " a", " country", " that", " has", " been", " itself", " devastated", " by", " a", " similar", " situation", ",", " we", " are", " absolutely", " saddened", " by", " what", "'s", " happening", " in", " Haiti", ",\"", " said", " Marty", " Nat", "ale", "g", "awa", ".", "\n", "\n", "12", "57", " Haitian", " DJ", " Care", "l", " Ped", "re", " tells", " BBC", "'s", " Haitian", " DJ", " Care", "l", " Ped", "re", " tells", " BBC", "'s", " New", "sh", "our", " he", " has", " seen", " a", " lot", " of", " dead", " bodies", " and", " collapsed", " buildings", ".", " \"", "I", "'ve"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 51, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" Pac", "-", "12", " Rec", "ru", "iter", " of", " the", " Year", ".", " Rivals", ".", "com", " tab", "bed", " Bloom", "gren", " as", " one", " of", " the", " nation", "'s", " top", "-", "25", " recruit", "ers", " in", " 2016", ".", " Football", "S", "co", "op", ".", "com", " named", " Bloom", "gren", " its", " 2013", " Offensive", " Line", " Coach", " of", " the", " Year", " following", " two", " years", " of", " eye", "-", "opening", " accomplishments", ".", "\n", "\n", "Bloom", "gren", " joined", " the", " Cardinal", " staff", " as", " offensive", " line", " coach", " and", " run", " game", " coordinator", " in", " 2011", " after", " spending", " four", " seasons", " with", " the", " New", " York", " Jets", ",", " where", " he", " served", " as", " assistant", " offensive", " coordinator", " (", "2010", "),", " offensive", " assistant", " (", "2009", ")", " and", " offensive", " quality", " control", " coach", " (", "2007", "-", "08", ").", "\n", "\n", "In", " 2013", ",", " all", " five", " offensive", " linemen", " received", " All", "-", "Pac", "-", "12", " honors", ","], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.285, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 118, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["ett", ",", " Stanford", "'s", " unanimous", " first", " team", " All", "-", "America", " --", " and", " only", " the", " ninth", " in", " school", " history", ".", " Garn", "ett", " was", " the", " program", "'s", " first", " Out", "land", " Trophy", " winner", " as", " the", " nation", "'s", " top", " interior", " lineman", ",", " and", " won", " the", " Morris", " Trophy", ",", " presented", " to", " the", " Pac", "-", "12", " Lin", "eman", " of", " the", " Year", ".", "\n", "\n", "Bloom", "gren", "'s", " linemen", " were", " named", " a", " final", "ist", " for", " the", " inaugural", " Joe", " Moore", " Award", " in", " 2015", ",", " given", " to", " the", " nation", "'s", " top", " offensive", " line", ".", "\n", "\n", "While", " facing", " six", " nationally", " ranked", " opponents", " in", " 2014", ",", " including", " four", " on", " the", " road", ",", " Bloom", "gren", " helped", " Stanford", " win", " eight", " games", ".", " The", " Cardinal", " scored", " at", " least", " 30", " points", " in", " seven", " of", " 13", " games", " on", " the", " season", " and", " increased"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.165, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 61, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" including", " first", "-", "team", " selection", " Yan", "key", ".", " Yan", "key", " was", " named", " Stanford", "'s", " first", " two", "-", "time", " first", "-", "team", " All", "-", "American", " selection", " since", " Bob", " Whit", "field", " (", "1990", "-", "91", ").", " One", " of", " four", " senior", " starters", " on", " the", " offensive", " line", " in", " 2013", ",", " Yan", "key", " was", " also", " selected", " as", " a", " semif", "inal", "ist", " for", " the", " Out", "land", " Trophy", " and", " Lomb", "ardi", " Award", ".", "\n", "\n", "Four", " of", " Bloom", "gren", "'s", " five", " starting", " offensive", " linemen", " earned", " All", "-", "Pac", "-", "12", " honors", " in", " 2012", ",", " including", " first", "-", "te", "amer", " Yan", "key", ".", "\n", "\n", "Yan", "key", " was", " the", " winner", " of", " the", " league", "'s", " 2012", " Morris", " Trophy", ".", " The", " consensus", " All", "-", "American", " was", " Stanford", "'s", " first", " winner", " of", " the", " award", " since", " 2002", ".", "\n"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.284, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 76, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" Smith", " at", " quarterback", " than", " with", " F", "oles", " or", " Vick", ".", "\n", "\n", "From", " Video", "\n", "\n", "Against", " the", " Bears", ",", " the", " Eagles", " used", " a", " tackle", "-", "over", " set", " with", " right", " tackle", " Lane", " Johnson", " positioned", " between", " left", " tackle", " Jason", " Peters", " and", " left", " guard", " Evan", " Math", "is", ".", " There", " was", " no", " motion", " man", ",", " so", " this", " was", " still", " not", " the", " Wild", "cat", ".", "\n", "\n", "From", " Video", "\n", "\n", "This", " was", " a", " classic", " power", " play", ".", " The", " linemen", " on", " the", " play", "-", "side", " were", " down", "-", "blocking", ".", " Running", " back", " Chris", " Polk", " was", " the", " lead", " blocker", " and", " did", " a", " nice", " job", " blocking", " the", " defensive", " tackle", ".", " Right", " guard", " Todd", " Her", "rem", "ans", " pulled", " to", " the", " left", " and", " entered", " the", " cre", "ase", " to", " block", " the", " plays", "ide", " linebacker", ".", "\n"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 75, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["Some", " in", " the", " audience", " are", " wearing", " surgical", " masks", ".", " There", " is", " a", " shirt", "less", " man", " pacing", " rings", "ide", ",", " presumably", " invested", " in", " the", " match", "'s", " outcome", ",", " though", " his", " presence", " is", " not", " remarked", " upon", ".", " He", "'s", " wearing", " a", " gigantic", " head", "piece", " reminiscent", " of", " Saur", "on", "'s", " battle", " helmet", ".", " It", " looks", " like", " a", " haunted", " castle", " is", " growing", " out", " of", " his", " shoulders", ".", "\n", "\n", "The", " outlook", " is", " grim", " for", " h", "agg", "ard", " old", " Vampire", " Chicken", ".", " He", " is", " facing", " a", " pretty", " boy", " hero", " wrestler", " with", " Masters", " of", " the", " Universe", " mus", "cul", "ature", ",", " blonde", "-", "stre", "aked", " boy", "-", "band", " hair", ",", " and", " a", " surg", "ically", " ideal", "ized", " face", ".", " Above", " a", " lantern", " jaw", ",", " the", " hero", "'s", " cheeks", " are", " round", " as", " a", " C", "abbage", " Patch"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 105, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" also", " presents", " issues", " when", " pass", "-", "blocking", " and", " running", " between", " the", " tackles", ".", " However", ",", " Williams", "\u00e2\u0122", "\u013b", " athleticism", " and", " ability", " to", " break", " off", " the", " big", " run", " makes", " him", " deserving", " of", " a", " chance", " to", " make", " the", " roster", " in", " Cincinnati", ".", "\n", "\n", "The", " Bengals", " selection", " of", " Joe", " M", "ixon", " was", " a", " big", " addition", ",", " but", " there", "\u00e2\u0122", "\u013b", "s", " still", " uncertainty", " in", " regards", " to", " when", " Giovanni", " Bernard", " will", " return", " from", " his", " ACL", " injury", ".", " On", " top", " of", " that", ",", " Jeremy", " Hill", "\u00e2\u0122", "\u013b", "s", " production", " has", " dropped", " off", " since", " his", " rookie", " season", ".", " Therefore", ",", " there", " is", " a", " legitimate", " opportunity", " for", " Williams", " sneak", " his", " way", " into", " a", " roster", " spot", ".", " If", " Williams", " can", " prove", " to", " be", " durable", ",", " he", " has", " the", " ability", " to", " be", " a", " productive", " change"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.281, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 71, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": ["ar", " ad", "em", "\u00c3\u00a1s", " que", " la", " demand", "a", " era", " de", " b", "aja", " cal", "idad", ".", " Por", " es", "o", " un", " camp", "es", "ino", " tur", "co", " que", " se", " pas", "aba", " el", " d", "\u00c3\u0143a", " fr", "at", "ach", "ando", " un", "a", " c", "ated", "ral", " por", " 60", " euros", " y", " dorm", "\u00c3\u0143a", " en", " un", " cont", "ened", "or", " pod", "\u00c3\u0143a", " viol", "arse", " a", " un", "a", " p", "ob", "re", " m", "uj", "er", " dro", "g", "ada", " y", " a", " ve", "ces", " at", "ada", " sin", " much", "as", " v", "u", "elt", "as", " al", " as", "un", "to", ".", "A", " la", " par", " del", " problem", "a", " surg", "ier", "on", " -", "cu", "\u00c3\u00a1", "nd", "o", " no", "-", " est", "ud", "ios", "os", " del", " f", "en", "\u00c3\u00b3", "men", "o", " que", " fu", "er", "on", " finan", "ci", "ados", " para", " tranquil", "iz", "ar", " las", " cul", "pos", "as", " al"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.275, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 91, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 0.8, "bin_contains": -1, "tokens": [" outline", " a", " variety", " of", " \u00e2\u0122", "\u013e", "un", "law", "ful", " acts", "\u00e2\u0122", "\u013f", " that", " could", " result", " in", " a", " citation", " or", " revoked", " license", ".", " The", " ordinance", " is", " aimed", " at", " making", " it", " harder", " for", " prostitution", " rings", " and", " other", " illegal", " outfits", " to", " thrive", " under", " the", " guise", " of", " the", " massage", " business", ".", "<|endoftext|>", "Get", " the", " biggest", " daily", " stories", " by", " email", " Subscribe", " Thank", " you", " for", " subscribing", " See", " our", " privacy", " notice", " Could", " not", " subscribe", ",", " try", " again", " later", " Invalid", " Email", "\n", "\n", "P", "up", "ils", " are", " still", " taught", " in", " Shakespeare", "\u00e2\u0122", "\u013b", "s", " historic", " 5", "87", "-", "year", "-", "old", " half", "-", "tim", "bered", " former", " classroom", " today", ".", "\n", "\n", "But", " if", " a", " \u00c2\u00a3", "1", "million", " Heritage", " Lot", "tery", " Fund", " bid", " is", " successful", ",", " one", " of", " the", " finest", " surviving", " old", " school", "rooms", " in", " Britain"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.27, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 120, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.64, "bin_max": 0.8, "bin_contains": 0.0, "tokens": ["\u2014", "t", "otally", ",", " completely", " flat", "?", "\n", "\n", "R", "is", "hel", " is", " very", " familiar", " with", " the", " Kansas", " flat", "ness", " question", ".", " He", " was", " a", " student", " of", " Mark", " F", "on", "stad", ",", " a", " Texas", " State", " ge", "ographer", " who", ",", " in", " 2003", ",", " set", " out", " with", " some", " colleagues", " and", " a", " laser", " microscope", " to", " determine", " which", " was", " fl", "atter", ":", " Kansas", " or", " an", " I", "H", "OP", " panc", "ake", ".", " The", " resulting", " study", ",", " titled", " \u00e2\u0122", "\u013e", "Kansas", " Is", " Fl", "atter", " Than", " a", " Panc", "ake", ",", "\u00e2\u0122", "\u013f", " likely", " added", " to", " the", " public", " misconceptions", " that", " rank", "le", " Dob", "son", " and", " Campbell", ".", " (", "They", " also", " point", " out", " that", ",", " if", " you", " use", " the", " particular", " mathematical", " approach", " of", " F", "on", "stad", " et", " al", ",", " \u00e2\u0122", "\u013e", "there", " is", " no", " place", " on"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.64, "bin_max": 0.8, "bin_contains": 0.0, "tokens": [" likely", " dict", "ating", " under", " the", " gun", ".", "\n", "\n", "And", ",", " lo", " and", " behold", ",", " on", " the", " list", " of", " ad", "her", "ing", " parties", ",", " A", "IG", " takes", " front", " and", " center", " stage", " (", "together", " with", " several", " other", " parties", " that", " probably", " deserve", " the", " microscope", " treatment", ").", "\n", "\n", "So", " -", " in", " simple", " terms", ",", " IS", "DA", ",", " which", " is", " the", " only", " effective", " supervisor", " of", " the", " Over", " The", " Counter", " C", "DS", " market", ",", " is", " giving", " its", " blessing", " for", " trades", " to", " occur", " (", "cross", ")", " below", " where", " there", " is", " a", " realistic", " market", " bid", ",", " or", " higher", " than", " the", " offer", ".", " In", " traditional", " equity", " markets", " this", " is", " a", " highly", " illegal", " practice", ".", " IS", "DA", " is", " allowing", " retrospective", " arbitrary", " trades", " to", " have", " occurred", " at", " whatever", " price", " any", " two", " parties", " agree", " on", ",", " so"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.797, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 41, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.64, "bin_max": 0.8, "bin_contains": 0.0, "tokens": [" would", " nonetheless", " suggest", " for", " complexity", " relevant", " to", " those", " types", ",", " the", " general", " case", " being", " made", " against", " conservatives", " in", " those", " areas", " needs", " revision", ".", " This", " would", " present", " to", " us", " a", " more", " nuanced", " and", " accurate", " picture", " of", " the", " relationship", " between", " ideology", " and", " complexity", " and", " suggest", " at", " the", " least", " that", " the", " current", " picture", " must", " be", " qualified", " by", " the", " type", " of", " complexity", " measurement", " under", " the", " microscope", ".", " The", " potential", " pitfalls", " of", " self", "\u00e2\u0122\u0132", "report", " measurement", " Further", ",", " for", " the", " purposes", " of", " determining", " the", " average", " complexity", " of", " a", " group", " of", " persons", ",", " the", " kinds", " of", " self", "\u00e2\u0122\u0132", "report", " measurements", " comprising", " the", " bulk", " of", " prior", " meta", "\u00e2\u0122\u0132", "an", "alyses", " have", " some", " additional", " potential", " pitfalls", " in", " interpretation", ".", " A", " self", "\u00e2\u0122\u0132", "report", " measurement", " is", " at", " best", " an", " indirect", " marker", " of", " a", " potential", " complexity", "\u00e2\u0122\u0132"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 61, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.48, "bin_max": 0.64, "bin_contains": 1e-05, "tokens": ["\u013e", "V", "irt", "ually", " every", " time", " Harvey", " was", " hired", " for", " a", " television", " show", ",", " [", "Co", "oper", "]", " would", " contact", " the", " owners", " or", " principals", " to", " inform", " them", " of", " potentially", " embarrassing", " material", " and", "/", "or", " tapes", " and", " attempt", " to", " have", " them", " influence", " Harvey", " to", " pay", " for", " the", " tapes", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Interestingly", ",", " court", " documents", " appear", " to", " show", " that", " Harvey", " admits", " to", " the", " r", "ants", ",", " saying", " that", " at", " times", " he", " was", " ed", "g", "ier", " than", " others", ".", " \u00e2\u0122", "\u013e", "I", " didn", "\u00e2\u0122", "\u013b", "t", " have", " to", " concern", " myself", " with", " branding", " or", " imaging", " or", " anything", ".", " You", " could", " just", " say", " \u2014", " I", " thought", " I", " was", " funn", "ier", ",", "\u00e2\u0122", "\u013f", " Harvey", " said", ".", "\n", "\n", "Co", "oper", " alleges", " that", " on", " one", " tape", ",", " Harvey", " says", " it"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.494, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 93, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.48, "bin_max": 0.64, "bin_contains": 1e-05, "tokens": ["%", " of", " their", " genome", " inherited", " from", " European", " ancestors", ".[", "173", "]", " If", " racial", " IQ", " gaps", " have", " a", " partially", " genetic", " basis", ",", " one", " might", " expect", " blacks", " with", " a", " higher", " degree", " of", " European", " ancestry", " to", " score", " higher", " on", " IQ", " tests", " than", " blacks", " with", " less", " European", " ancestry", ",", " because", " the", " genes", " inherited", " from", " European", " ancestors", " would", " likely", " include", " some", " genes", " with", " a", " positive", " effect", " on", " IQ", ".", " Genetic", "ist", " Alan", " Temple", "ton", " has", " argued", " that", " an", " experiment", " based", " on", " the", " Mend", "el", "ian", " \"", "common", " garden", "\"", " design", " where", " specimens", " with", " different", " hybrid", " compositions", " are", " subjected", " to", " the", " same", " environmental", " influences", ",", " would", " be", " the", " only", " way", " to", " definitively", " show", " a", " causal", " relation", " between", " genes", " and", " IQ", ".", " Sum", "mar", "izing", " the", " findings", " of", " adm", "ixture", " studies", ",", " he", " concludes"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.51, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 86, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.48, "bin_max": 0.64, "bin_contains": 1e-05, "tokens": [" guidance", " and", " navigation", " systems", ".", " (", "Dr", ".", " Kh", "alf", "in", "\u00e2\u0122", "\u013b", "s", " bio", " is", " below", " the", " video", ".)", "\n", "\n", "I", "gor", " Kh", "alf", "in", ",", " PhD", "\n", "\n", "Dr", ".", " Kh", "alf", "in", " has", " over", " 30", " years", " of", " experience", " in", " the", " aerospace", "/", "defense", " industries", " and", " academia", ",", " holds", " 5", " patents", " in", " electromagnetic", " tracking", " and", " has", " published", " over", " 40", " scientific", " papers", " on", " related", " topics", ".", " His", " expertise", " includes", " opt", "oe", "lect", "ronics", ",", " electrom", "ag", "net", "ics", ",", " imaging", " systems", " and", " sensors", ",", " remote", " sensing", ",", " applied", " physics", ",", " signal", " processing", ",", " and", " non", "linear", " systems", ".", " Dr", ".", " Kh", "alf", "in", " is", " currently", " employed", " with", " Lockheed", " Martin", " Space", " Systems", " Company", ",", " where", " he", " leads", " the", " projects", " on", " spacecraft", " control", " systems", ".", " His", " prior"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 81, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.48, "bin_max": 0.64, "bin_contains": 1e-05, "tokens": ["Man", "l", "ius", "\n", "\n", "Editor", "ial", " failed", " to", " address", " privacy", " and", " civil", " liberties", " issues", "\n", "\n", "To", " the", " Editor", ":", "\n", "\n", "It", " is", " readily", " understandable", " that", " news", " corporations", " would", " want", " to", " use", " drones", " to", " record", " and", " report", " news", ".", " The", " imaging", " capabilities", " of", " drones", " are", " mighty", " impressive", ".", "\n", "\n", "Yes", ",", " drones", " might", " \"", "enh", "ance", " the", " public", "'s", " understanding", " of", " events", ".\"", " And", ",", " true", ",", " drones", " may", " well", " have", " been", " \"", "used", " to", " capture", " footage", " of", " massive", " demonstrations", " in", " Ukraine", ".\"", " (", "Indeed", ",", " drones", " are", " ideal", " for", " identifying", " and", " monitoring", " demonstrators", ".)", "\n", "\n", "However", ",", " your", " May", " 11", " editorial", ",", " \"", "Journal", "ists", " have", " right", " to", " use", " drones", ",", " too", ",\"", " under", " analyzed", " the", " issue", ".", " Sure", ",", " news", " corporations"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.498, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 42, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.32, "bin_max": 0.48, "bin_contains": 1e-05, "tokens": [" darker", " aspect", " of", " this", " kind", " of", " cultural", " pathology", ",", " just", " as", " there", " are", " serious", " analyses", " pointing", " to", " the", " socially", " toxic", " effects", " of", " the", " JFK", " assassination", " \u00e2\u0122", "\u013e", "altern", "ate", " theories", "\u00e2\u0122", "\u013f", ".", " For", " space", "flight", ",", " being", " distracted", " by", " the", " wrong", " cause", " means", " being", " tempted", " by", " the", " wrong", " fix", ".", " That", "\u00e2\u0122", "\u013b", "s", " never", " amusing", ",", " and", " often", " can", " be", " expensive", ".", " For", " space", "flight", ",", " being", " distracted", " by", " the", " wrong", " cause", " means", " being", " tempted", " by", " the", " wrong", " fix", ".", " That", "\u00e2\u0122", "\u013b", "s", " never", " amusing", ",", " and", " often", " can", " be", " expensive", ".", " As", " an", " egregious", " \u00e2\u0122", "\u013e", "bad", " example", "\u00e2\u0122", "\u013f", " of", " wrong", " causes", ",", " a", " recent", " book", " (", "Dark", " Mission", ",", " by", " Richard", " Ho", "ag", "land", " and", " Michael", " Bar", "a", ")", " spent", " a"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.368, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 7, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.32, "bin_max": 0.48, "bin_contains": 1e-05, "tokens": ["),", " Devin", " T", "oner", " (", "L", "ans", "down", "e", "/", "Le", "in", "ster", "),", " Paul", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "Young", " Mun", "ster", ")", " (", "capt", "),", " Peter", " O", "\u00e2\u0122", "\u013b", "Mah", "ony", " (", "C", "ork", " Constitution", "/", "M", "un", "ster", "),", " Sean", " O", "\u00e2\u0122", "\u013b", "Brien", " (", "U", "CD", "/", "Le", "in", "ster", "),", " Jamie", " He", "as", "lip", " (", "Dub", "lin", " University", "/", "Le", "in", "ster", ").", " Repl", "acements", ":", " Ian", " Mad", "igan", " (", "Black", "rock", " College", "/", "Le", "in", "ster", ")", " for", " Se", "xton", " (", "28", " mins", "),", "\n", "\n", "I", "ain", " Henderson", " (", "B", "ally", "nah", "inch", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b", "Connell", " (", "half", "=", "time", "),", " Chris", " Henry", " (", "Mal", "one", "/", "Ul", "ster", ")", " for", " O", "\u00e2\u0122", "\u013b"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.32, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 120, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.32, "bin_max": 0.48, "bin_contains": 1e-05, "tokens": [" shield", " (~", "$", "2", ".", " This", " slides", " over", " a", " fluorescent", " bulb", " to", " prevent", " it", " from", " shattering", ")", " Small", " piece", " of", " aluminum", " foil", ".", " (", "Had", " it", " in", " the", " house", ",", " because", " we", "\u00e2\u0122", "\u013b", "re", " civilized", ")", "\n", "\n", "The", " Steps", "\n", "\n", "I", " redesigned", " the", " PVC", " pipe", " and", " made", " it", " about", " 5", "\u00e2\u0122\u00b3", " shorter", ".", " It", " was", " too", " long", " and", " unw", "ield", "y", ".", " I", " took", " a", " little", " more", " care", " in", " marking", " off", " the", " space", " to", " cut", " out", ".", " The", " blue", " tape", " worked", " perfect", " to", " keep", " a", " straight", " line", ".", "\n", "\n", "Cut", " the", " opening", " in", " the", " PVC", " pipe", ".", " This", " can", " be", " whatever", " width", " you", " want", ".", " Mine", " is", " a", " little", " less", " than", " halfway", " through", ".", "\n", "\n", "Cut", " the", " fl", "ou", "rescent", " shield", " to"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.354, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 9, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.32, "bin_max": 0.48, "bin_contains": 1e-05, "tokens": [" the", " same", " length", " as", " the", " pipe", ".", " Then", " use", " a", " utility", " knife", " to", " cut", " it", " long", "ways", ".", "\n", "\n", "Sand", " the", " inside", " and", " outside", " of", " the", " fluorescent", " shield", " to", " create", " a", " \u00e2\u0122", "\u013e", "sm", "oked", " glass", "\u00e2\u0122", "\u013f", " look", " and", " set", " aside", ".", " This", " shows", " how", " I", " got", " the", " fluorescent", " shield", " to", " stay", " large", " enough", " to", " snug", "ly", " fit", " into", " the", " pipe", ".", " Just", " a", " simple", " slit", " on", " one", " side", " and", " the", " other", " side", " slides", " in", ".", " Also", " you", " can", " see", " how", " the", " once", " clear", " plastic", " looks", " after", " sand", "ing", " it", ":", "\n", "\n", "Remove", " the", " \u00e2\u0122", "\u013e", "c", "rown", "\u00e2\u0122", "\u013f", " off", " of", " the", " bulb", " end", " of", " the", " flashlight", ".", " You", " will", " still", " have", " the", " reflect", "or", " cone", " and", " clear", " focus", "er", " inside", " the", " light"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.345, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.32, "bin_max": 0.48, "bin_contains": 1e-05, "tokens": ["We", " take", " elements", " of", " the", " scans", " and", " then", " we", " add", " to", " them", ",\"", " explains", " Yu", ".", " \"", "When", " we", " want", " a", " particular", " shape", ",", " we", " add", " pose", "-", "based", " deform", "ers", ".", " We", " can", " sculpt", " the", " exact", " shape", " we", " want", " all", " over", " the", " body", ":", " on", " the", " knees", ",", " shoulders", ",", " elbows", ",", " the", " waist", ".", " \"", "In", " addition", " to", " those", " pose", "-", "based", " deform", "ers", ",", " we", " add", " wr", "inkle", " maps", " as", " well", ".", " When", " she", " bends", " a", " joint", ",", " we", " actually", " see", " the", " wrinkles", " in", " her", " shirt", " change", ".", " Or", " if", " she", " bends", " her", " arm", " you", "\u00e2\u0122", "\u013b", "ll", " see", " the", " mus", "cul", "ature", " in", " her", " shoulders", " deform", ".\"", " Yu", "\u00e2\u0122", "\u013b", "s", " background", " is", " in", " anatomical", " art", " and", " medical", " illustrations", ".", " Anat", "omical", " models"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.098, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.34, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 118, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.16, "bin_max": 0.32, "bin_contains": 8e-05, "tokens": ["\u00c3\u00a1", "-", "Vis", " Society", ",", " which", " applies", " scientific", " rig", "or", " to", " crowd", "-", "s", "ourced", " poems", ",", " often", " employing", " large", " crowds", " at", " parties", " to", " write", ",", " Mad", " Lib", "s", "-", "style", ",", " a", " series", " of", " poems", " about", " love", " and", " longing", ".", " No", " other", " poets", " in", " town", ",", " likely", ",", " have", " dissect", "ed", " a", " poem", " into", " pie", " charts", " on", " a", " white", "board", " while", " wearing", " a", " lab", " coat", ".", "\n", "\n", "On", " her", " own", ",", " Nelson", " loves", " to", " tease", " out", " the", " poetry", " in", " science", ",", " finding", " resonance", " in", " the", " long", " and", " mysterious", " Latin", " words", " and", " phrases", " we", "\u00e2\u0122", "\u013b", "ve", " used", " to", " name", " the", " world", ".", " One", " of", " my", " favorites", " of", " her", " poems", " is", " \u00e2\u0122", "\u013e", "The", " First", " Photograph", ",", "\u00e2\u0122", "\u013f", " which", " explains", " the", " process", " that", " created", " a"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.241, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.16, "bin_max": 0.32, "bin_contains": 8e-05, "tokens": [" Miami", " but", " my", " mind", " is", " in", " Haiti", ",\"", " Fletcher", " T", "ous", "s", "aint", ",", " a", " young", " immigrant", ",", " tells", " the", " BBC", ".", "\n", "\n", "13", "10", " Two", " days", " after", " the", " disaster", " struck", ",", " people", " speak", " of", " still", " hearing", " voices", " crying", " from", " the", " rubble", ".", " Two", " days", " after", " the", " disaster", " struck", ",", " people", " speak", " of", " still", " hearing", " voices", " crying", " from", " the", " rubble", ".", "\n", "\n", "130", "9", " Google", " and", " Ge", "oe", "ye", " have", " put", " together", " some", " Google", " and", " Ge", "oe", "ye", " have", " put", " together", " some", " new", " satellite", " images", " of", " Haiti", ",", " taken", " after", " the", " quake", " and", " showing", " the", " extent", " of", " the", " destruction", ".", "\n", "\n", "130", "3", " The", " foreign", " minister", " of", " Indonesia", " -", " a", " country", " which", " has", " suffered", " natural", " disasters", " in", " the", " past", " -", " expressed", " condolences", " to", " Haiti"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.314, 0.0, 0.0], "qualifying_token_index": 124, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.16, "bin_max": 0.32, "bin_contains": 8e-05, "tokens": [" overfl", "ows", " that", " an", " auxiliary", " dynamic", " alloc", "ator", " is", " used", ".", " The", " overflow", " allocation", " feature", " is", " best", " used", " for", " pathological", " cases", " or", " during", " development", " when", " the", " container", "'s", " fixed", " size", " is", " being", " tuned", ".", " All", " fixed", " containers", " have", " high", "-", "water", " mark", " tracking", " to", " assist", " in", " size", " tuning", ".", " The", " following", " is", " the", " template", " declaration", " for", " fixed", "_", "vector", ":", " template", " <", "ty", "pen", "ame", " T", ",", " size", "_", "t", " node", "Count", ",", " bool", " enable", "Over", "flow", " =", " true", ",", " typ", "ename", " All", "oc", "ator", " overflow", "All", "oc", "ator", " =", " E", "AST", "L", "All", "oc", "ator", ">", " class", " fixed", "_", "vector", " {", " ...", " };", " fixed", "_", "sub", "string", "\n", "\n", "This", " is", " a", " string", " which", " is", " a", " view", " on", " an", " arbitrary", " span", " of", " characters", ".", " Thus", " if"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 19, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.16, "bin_max": 0.32, "bin_contains": 8e-05, "tokens": ["un", "ament", "e", " of", "rec", "idas", " por", "P", "or", " t", "anto", ",", " si", " qu", "ie", "ren", " put", "ear", ",", " p", "ute", "en", " al", " fiscal", " de", " Ju", "icio", " y", " a", " la", " Pro", "cur", "aci", "\u00c3\u00b3n", " (", "que", " son", " el", " ver", "dad", "ero", " b", "raz", "o", " del", " P", "oder", " Pol", "\u00c3\u0143", "t", "ico", ")", " y", " no", " a", " los", " j", "ue", "ces", " a", " los", " que", " les", " l", "le", "gan", " las", " pr", "ue", "bas", " coc", "in", "adas", " y", " deb", "en", " dec", "id", "ir", " so", "bre", " su", " ver", "ac", "idad", ".", " Principal", "ment", "e", " en", " un", " cas", "o", " en", " que", ",", " com", "o", " se", " di", "jo", " no", " se", " p", "udo", " comp", "oner", " la", " pr", "ue", "ba", " con", " document", "ales", ",", " ni", " n", "ada", ".", " S", "\u00c3\u00b3", "lo", " testim", "onial", "es", "..", " La"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.212, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 62, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.16, "bin_max": 0.32, "bin_contains": 8e-05, "tokens": [" of", " the", " tooth", ".", "\n", "\n", "Mr", " Per", "ren", "oud", " said", " he", " is", " \"", "pretty", " confident", "\"", " that", " his", " team", " will", " find", " more", " human", " fossils", " from", " this", " period", " on", " the", " site", ".", " \"", "Our", " daily", " life", " is", " to", " determine", " what", " human", " activities", " looked", " like", " 560", ",", "000", " years", " ago", ",\"", " he", " said", ".", "\n", "\n", "Professor", " Chris", " String", "er", ",", " merit", " researcher", " in", " human", " origins", " at", " the", " Natural", " History", " Museum", " of", " London", ",", " wrote", " in", " an", " email", " to", " the", " AP", " that", " \"", "well", "-", "dated", " teeth", " of", " this", " age", " are", " very", " important", " as", " they", " probably", " belonged", " to", " the", " species", " Homo", " he", "idel", "berg", "ensis", ",", " which", " is", " already", " known", " from", " Ar", "ago", " (", "in", " T", "aut", "a", "vel", ")", " in", " France", ",", " M", "auer", " in", " Germany", " and"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 24, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.16, "bin_contains": 0.9998999999999999, "tokens": ["\u013e", "Robert", " de", " Cour", "cy", " L", "aff", "an", " was", " a", " great", " friend", " of", " the", " Flower", " family", " who", " helped", " to", " restore", " the", " school", " building", " in", " the", " 1890", "s", ".", " He", " was", " a", " great", " education", " reform", "er", ".", " He", ",", " head", " boy", " Ralph", " Gar", "lic", " and", " a", " representative", " from", " Shakespeare", "\u00e2\u0122", "\u013b", "s", " Birth", "place", " decided", " they", " would", " walk", " from", " the", " school", " to", " Holy", " Trinity", " Church", " and", "\n", "\n", "lay", " flowers", " on", " the", " grave", ".", " The", " following", " year", " was", " the", " beginning", " of", " what", " has", " become", " the", " annual", " procession", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "There", " had", " been", " a", " small", " parade", " by", " coun", "sell", "ors", " in", " 18", "10", ",", " but", " the", " school", " initiated", " the", " procession", ".", " It", " was", " reported", " in", " the", " Times", " of", " London", ",", " newspapers", " in", " Strat", "ford", ",", " Le"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 58, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.16, "bin_contains": 0.9998999999999999, "tokens": ["7", ".", "38", "\n", "\n", "8", ".", "84", "\n", "\n", "1", ".", "06", "\n", "\n", "0", ".", "88", "\n", "\n", "1", ".", "00", "\n", "\n", "1", ".", "01", "\n", "\n", "1", ".", "10", "\n", "\n", "0", ".", "89", "\n", "\n", "0", ".", "83", "\n", "\n", "0", ".", "82", "\n", "\n", "1", ".", "47", "\n", "\n", "0", ".", "70", "\n", "\n", "1", ".", "45", "\n", "\n", "1", ".", "07", "\n", "\n", "1", ".", "47", "\n", "\n", "0", ".", "96", "\n", "\n", "1", ".", "20", "\n", "\n", "0", ".", "96", "\n", "\n", "0", ".", "77", "\n", "\n", "1", ".", "07", "\n", "\n", "0", ".", "95", "\n", "\n", "1", ".", "13", "\n", "\n", "0", ".", "97", "\n", "\n", "1", ".", "13", "\n", "\n", "1", ".", "25", "\n", "\n", "0", "."], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 74, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.16, "bin_contains": 0.9998999999999999, "tokens": ["ton", ",", " Nevada", ",", " on", " 3", " September", " 2007", " on", " a", " flight", " that", " was", " expected", " to", " last", " three", " hours", ".", " In", " 2002", ",", " he", " became", " the", " first", " person", " to", " circle", " the", " globe", " solo", " in", " a", " balloon", " and", " had", " about", " 100", " other", " world", " records", " to", " his", " name", ".", "\n", "\n", "Click", " here", " to", " return", "\n", "\n", "E", "-", "mail", " this", " to", " a", " friend", " Print", "able", " version", " Book", "mark", " with", ":", " Delicious", "\n", "\n", "D", "igg", "\n", "\n", "reddit", "\n", "\n", "Facebook", "\n", "\n", "St", "umble", "Upon", " What", " are", " these", "?", "<|endoftext|>", "Joseph", " Er", "l", "anger", " (", "January", " 5", ",", " 18", "74", " \u2013", " December", " 5", ",", " 1965", ")", " was", " an", " American", " phys", "iologist", " who", " is", " best", " known", " for", " his", " contributions", " to", " the", " field", " of", " neuroscience", ".", " Together", " with", " Herbert", " Spencer"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 81, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.16, "bin_contains": 0.9998999999999999, "tokens": ["onda", " Rouse", "y", ",", " on", " wanting", " to", " have", " kids", "\n", "\n", "RR", ":", " It", " can", " be", " creepy", " and", " weird", ".", " Personally", ",", " I", " wasn", "'t", " allowed", " to", " have", " a", " cellphone", " until", " I", " was", " 16", ".", " I", " think", " having", " a", " cellphone", " becomes", " a", " social", " cr", "utch", ",", " especially", " during", " those", " uncomfortable", " puberty", " years", ".", " If", " I", " had", " a", " kid", ",", " I", " would", " try", " to", " keep", " them", " out", " of", " social", " media", " until", " they", " were", " at", " least", " in", " their", " teens", ".", "\n", "\n", "AG", ":", " Do", " you", " want", " to", " have", " kids", " some", " day", "?", "\n", "\n", "RR", ":", " Definitely", ".", " I", "'m", " an", " ovarian", " gold", "mine", ".", " I", " can", "'t", " waste", " these", " genes", ".", "\n", "\n", "AG", ":", " Would", " you", " rather", " lose", " an", " arm", " or", " a", " leg", "?", "\n", "\n"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 114, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.16, "bin_contains": 0.9998999999999999, "tokens": [" are", " predicted", " by", " the", " bio", "ec", "ological", " hypothesis", " \u2013", " that", " gen", "otypes", " are", " transformed", " into", " phen", "otypes", " through", " non", "add", "itive", " synerg", "istic", " effects", " of", " the", " environment", ".[", "115", "]", " N", "is", "bett", " et", " al", ".", " (", "2012", ")", " suggest", " that", " high", " S", "ES", " individuals", " are", " more", " likely", " to", " be", " able", " to", " develop", " their", " full", " biological", " potential", ",", " whereas", " low", " S", "ES", " individuals", " are", " likely", " to", " be", " hindered", " in", " their", " development", " by", " adverse", " environmental", " conditions", ".", " The", " same", " review", " also", " points", " out", " that", " adoption", " studies", " generally", " are", " biased", " towards", " including", " only", " high", " and", " high", " middle", " S", "ES", " adoptive", " families", ",", " meaning", " that", " they", " will", " tend", " to", " overest", "imate", " average", " genetic", " effects", ".", " They", " also", " note", " that", " studies", " of", " adoption", " from", " lower", "-", "class", " homes", " to", " middle", "-"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 2, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}]}, {"feature_index": 3, "neuron_alignment_indices": [266, 480, 87], "neuron_alignment_values": [0.108, 0.108, 0.107], "neuron_alignment_l1": [0.005, 0.005, 0.005], "correlated_neurons_indices": [599, 170, 14], "correlated_neurons_l1": [0.014, 0.015, 0.014], "correlated_neurons_pearson": [0.014, 0.014, 0.013], "correlated_features_indices": [3, 2], "correlated_features_l1": [0.0, 0.0], "correlated_features_pearson": [0.0, -0.0], "neg_str": ["sharp", " MPG", " Rin", " ..........", "jab", "iny", "ran", " Watt", "geon", "rays"], "neg_values": [-0.675, -0.623, -0.622, -0.619, -0.601, -0.597, -0.596, -0.588, -0.583, -0.58], "pos_str": [" contingent", " conting", " contingency", " conditioned", " conditional", " delegation", " cohort", " detachment", " provisional", " facult"], "pos_values": [4.019, 2.235, 1.83, 1.382, 1.339, 1.337, 1.03, 0.986, 0.921, 0.895], "frac_nonzero": 0.00023, "freq_hist_data_bar_values": [0.035, 0.103, 0.172, 0.241, 0.309, 0.378, 0.446, 0.515, 0.583, 0.652, 0.721, 0.789, 0.858, 0.926, 0.995, 1.063, 1.132, 1.2, 1.269, 1.338, 1.406, 1.475, 1.543, 1.612, 1.68, 1.749, 1.818, 1.886, 1.955, 2.023, 2.092, 2.16, 2.229, 2.298, 2.366, 2.435, 2.503, 2.572, 2.64, 2.709, 2.778, 2.846, 2.915, 2.983, 3.052, 3.12, 3.189, 3.257, 3.326, 3.395], "freq_hist_data_bar_heights": [73, 32, 21, 1, 4, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8], "logits_hist_data_bar_heights": [9, 65, 428, 1572, 4283, 8019, 10177, 9411, 7377, 4599, 2474, 1111, 444, 172, 70, 31, 6, 2, 1, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1], "logits_hist_data_bar_values": [-0.628, -0.534, -0.441, -0.347, -0.253, -0.159, -0.065, 0.029, 0.123, 0.217, 0.31, 0.404, 0.498, 0.592, 0.686, 0.78, 0.874, 0.968, 1.062, 1.155, 1.249, 1.343, 1.437, 1.531, 1.625, 1.719, 1.813, 1.906, 2.0, 2.094, 2.188, 2.282, 2.376, 2.47, 2.564, 2.658, 2.751, 2.845, 2.939, 3.033, 3.127, 3.221, 3.315, 3.409, 3.502, 3.596, 3.69, 3.784, 3.878, 3.972], "n_prompts_total": 5000, "n_tokens_in_prompt": 128, "dataset": "Skylion007/openwebtext", "decoder_weights_dist": [], "activations": [{"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" the", " strategic", " channel", " at", " high", " risk", " of", " \u00e2\u0122", "\u013e", "war", ",", " strikes", ",", " terrorism", " and", " related", " per", "ils", "\u00e2\u0122", "\u013f", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "But", " it", "\u00e2\u0122", "\u013b", "s", " just", " a", " recommendation", ",", " and", " some", " under", "writers", " may", " not", " follow", " it", " for", " their", " very", " important", " clients", ",", "\u00e2\u0122", "\u013f", " Muk", "und", "an", " told", " Reuters", ".", " \u00e2\u0122", "\u013e", "Cost", "s", " have", " not", " gone", " up", ".", " Of", " course", ",", " if", " you", " are", " hijacked", " they", " go", " up", " quite", " significantly", ".", " But", " there", " is", " no", " contingent", " cost", " to", " piracy", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "S", "om", "ali", " pirates", " are", " currently", " holding", " about", " 130", " crew", " members", " hostage", " on", " at", " least", " seven", " vessels", ",", " including", " huge", " chemical", " tank", "ers", " and", " bulk", "-", "car", "riers", ".", " Gun", "men", " are", " holding", " vessels", " from", " Japan"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 82, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" gradient", " is", " getting", " st", "eeper", ",", " not", " only", " between", " contingent", " labor", " and", " the", " tenure", " track", ",", " and", " junior", " and", " senior", " faculty", " within", " the", " latter", ",", " but", " between", " institutions", " as", " well", ".", " Prof", "essors", " at", " doctoral", "-", "gr", "anting", " universities", " not", " only", " get", " paid", " a", " lot", " more", " than", " their", " colleagues", " at", " other", " four", "-", "year", " schools", ";", " the", " difference", " is", " growing", ",", " from", " 17", " percent", " in", " 1984", " to", " 28", " percent", " in", " 2003", ".", " (", "Their", " advantage", " over", " professors", " at", " community", " colleges", " increased", " during", " the", " same", " period", " from", " 33", " percent", " to", " 49", " percent", ".)", " The", " rich", " are", " getting", " richer", ".", " In", " 1970", " (", "it", " seems", " like", " an", " alternative", " universe", " now", ")", " faculty", " at", " public", " colleges", " and", " universities", " actually", " made", " about", " 10", " percent", " more", " than", " those", " at", " private", " schools", "."], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 9, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": ["el", "tering", " gym", " bed", "e", "ck", "ed", " with", " championship", " banners", ",", " the", " governor", " is", " received", " by", " a", " bo", "ister", "ous", " contingent", " of", " his", " old", " friends", " from", " the", " class", " of", " 1980", ".", " \u00e2\u0122", "\u013e", "Lots", " of", " people", " have", " asked", " me", " over", " the", " course", " of", " last", " week", ",", " why", " here", "?", "\u00e2\u0122", "\u013f", " he", " says", ".", " \u00e2\u0122", "\u013e", "Why", " here", "?", " Because", " everything", " started", " here", " for", " me", ".", " The", " confidence", ".", " The", " education", ".", " The", " friends", ".", " The", " family", ".", " And", " the", " love", " that", " I", "\u00e2\u0122", "\u013b", "ve", " always", " felt", " for", " and", " from", " this", " community", ".", "\u00e2\u0122", "\u013f", " Outside", " the", " gym", ",", " protesters", " pick", "et", " the", " speech", ",", " waving", " signs", " that", " read", " B", "ULL", "Y", ".", "\n", "\n", "On", " the", " campaign", " trail", ",", " he", " keeps", " getting", " incred", "ulous", " questions"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 20, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" than", " twenty", " years", ",", " to", " about", " two", "-", "thirds", " of", " what", " it", " was", " in", " 1980", ".", " The", " average", " six", "-", "year", " graduation", " rate", " at", " state", " schools", " is", " now", " a", " dismal", " 60", " percent", ",", " a", " function", " of", " class", " size", " and", " availability", ",", " faculty", " accessibility", ",", " the", " use", " of", " contingent", " instructors", " and", " other", " budget", "-", "related", " issues", ".", " Private", " universities", " actually", " lobby", " against", " public", " funding", " for", " state", " schools", ",", " which", " they", " see", " as", " competitors", ".", " In", " any", " case", ",", " a", " large", " portion", " of", " state", " scholarship", " aid", " goes", " to", " students", " at", " private", " colleges", " (", "in", " some", " cases", ",", " more", " than", " half", ")\u2014", "a", " kind", " of", " voucher", " system", " for", " higher", " education", ".", "\n", "\n", "Meanwhile", ",", " public", " universities", " have", " been", " shifting", " their", " financial", " aid", " criteria", " from", " need", " to", " merit", " to", " attract"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.42, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 47, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": ["ids", " around", " 1970", " and", " didn", "\u00e2\u0122", "\u013b", "t", " recover", " for", " almost", " thirty", " years", ".", " It", "\u00e2\u0122", "\u013b", "s", " no", " surprise", " that", " the", " percentage", " of", " college", " freshmen", " expressing", " an", " interest", " in", " academia", " was", " more", " than", " three", " times", " higher", " in", " 1966", " than", " it", " was", " in", " 2004", ".", "\n", "\n", "But", " the", " answer", " now", " is", " not", " to", " raise", " professors", "\u00e2\u0122", "\u013b", " salaries", ".", " Prof", "essors", " already", " make", " enough", ".", " The", " answer", " is", " to", " hire", " more", " professors", ":", " real", " ones", ",", " not", " academic", " lettuce", "-", "p", "ickers", ".", "\n", "\n", "Yet", " that", "\u00e2\u0122", "\u013b", "s", " the", " last", " thing", " schools", " are", " apt", " to", " do", ".", " What", " we", " have", " seen", " instead", " over", " the", " past", " forty", " years", ",", " in", " addition", " to", " the", " raising", " of", " a", " reserve", " army", " of", " contingent", " labor", ",", " is", " a", " kind"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.419, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 121, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" if", " we", "\u00e2\u0122", "\u013b", "re", " going", " to", " reach", " the", " oft", "-", "repe", "ated", " goal", " of", " universal", " post", "secondary", " education", ",", " an", " objective", " that", " would", " double", " enroll", "ments", "\u2014", "we", "\u00e2\u0122", "\u013b", "re", " going", " to", " need", " a", " lot", " more", " teachers", ":", " well", " paid", ",", " institution", "ally", " supported", ",", " socially", " valued", ".", " As", " of", " 2003", " there", " were", " about", " 400", ",", "000", " tenure", "-", "track", " professors", " in", " the", " United", " States", " (", "as", " compared", " with", " about", " 6", " million", " primary", "-", " and", " secondary", "-", "school", " teachers", ").", " Between", " reducing", " class", " sizes", ",", " reversing", " the", " shift", " to", " contingent", " labor", " and", " beef", "ing", " up", " our", " college", "-", "com", "pletion", " rates", ",", " we", "\u00e2\u0122", "\u013b", "re", " going", " to", " need", " at", " least", " five", " times", " as", " many", ".", "\n", "\n", "So", " where", "\u00e2\u0122", "\u013b", "s", " the", " money"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 91, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": ["about", " global", " warming", ",", " or", " the", " historical", " Jesus", ",", " or", " the", " way", " we", " raise", " our", " children", ".", " That", "\u00e2\u0122", "\u013b", "s", " why", " they", " need", " to", " have", " special", " protections", ".", "\n", "\n", "*", " *", " *", "\n", "\n", "But", " the", " tenure", " system", ",", " which", " is", " already", " being", " eroded", " by", " the", " growth", " of", " contingent", " labor", ",", " is", " not", " the", " only", " thing", " that", " is", " under", " assault", " in", " the", " top", "-", "down", ",", " corpor", "at", "ized", " academy", ".", " As", " Cary", " Nelson", " explains", " in", " No", " University", " Is", " an", " Island", " (", "2010", "),", " shared", " governance", "\u2014", "the", " principle", " that", " universities", " should", " be", " controlled", " by", " their", " faculties", ",", " which", " protects", " academic", " values", " against", " the", " enc", "roach", "ments", " of", " the", " spreadsheet", " brigade", "\u2014", "is", " also", " threatened", " by", " the", " changing", " structure", " of", " academic", " work", ".", " Cont", "ing"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [".", "7", ".", "2014", ")", " with", " two", " dozen", " top", " managers", " in", " tow", ",", " including", " five", " heads", " of", " DA", "X", "-", "listed", " companies", ".", " Since", " Tuesday", ",", " the", " final", " day", " of", " Merkel", "'s", " China", " visit", ",", " it", "'s", " been", " evident", " that", " the", " large", " German", " business", " contingent", " made", " less", " of", " an", " impression", " on", " the", " Chinese", " than", " some", " of", " the", " members", " of", " the", " delegation", " had", " hoped", ".", "\n", "\n", "In", " the", " end", ",", " the", " Chinese", " were", " not", " quite", " as", " open", " to", " everything", " the", " German", " business", " leaders", " had", " on", " their", " wish", " list", " as", " the", " Germans", " would", " have", " liked", ".", " Acc", "ords", " amount", "ing", " to", " 3", " billion", " euros", " ($", "4", " billion", ")", " were", " signed", ",", " which", " is", " not", " bad", " at", " all", ".", " However", ",", " it", "'s", " a", " relatively", " small", " package", " if", " you"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 44, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" void", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "If", " we", " can", " find", " a", " way", " to", " achieve", " greater", " revenue", " stability", " without", " the", " use", " of", " the", " contingency", " account", " \u2014", " though", " it", "\u00e2\u0122", "\u013b", "s", " probably", " not", " possible", " \u2014", " that", " would", " be", " my", " first", " desire", ",", "\u00e2\u0122", "\u013f", " said", " the", " Calgary", " MLA", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Phil", "os", "oph", "ically", ",", " I", " think", " conting", "encies", " are", " there", " to", " smooth", " things", " out", ".", " They", "\u00e2\u0122", "\u013b", "re", " not", " there", " to", " totally", " drain", " the", " bank", " account", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Although", " the", " price", " of", " oil", " has", " stayed", " below", " $", "60", " US", " per", " barrel", " for", " much", " of", " this", " year", ",", " Cec", "i", " said", " he", " hoped", " new", " corporate", " tax", " increases", " and", " hikes", " in", " personal", " income", " tax", " for", " the", " wealthiest", " Albert", "ans", " will", " meet", " some"], "values": [0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 59, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" red", ".", "\n", "\n", "The", " 2014", "-", "15", " consolidated", " surplus", " of", " $", "1", ".", "1", " billion", " ends", " of", " string", " of", " six", " consecutive", " deficits", " and", " boosts", " the", " province", "\u00e2\u0122", "\u013b", "s", " contingency", " fund", " to", " $", "8", ".", "2", " billion", " \u2014", " a", " reserve", " Finance", " Minister", " Joe", " Cec", "i", " says", " will", " come", " in", " handy", " in", " dealing", " with", " the", " drop", " in", " non", "-", "ren", "ew", "able", " resource", " revenue", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Going", " forward", " the", " climate", " \u2014", " the", " economy", " today", " \u2014", " is", " very", " different", " from", " what", " it", " was", " then", ",", "\u00e2\u0122", "\u013f", " he", " told", " reporters", " at", " the", " legislature", " Tuesday", " as", " the", " NDP", " government", " released", " the", " final", " year", "-", "end", " numbers", ".", "\n", "\n", "But", " Cec", "i", " said", " he", " was", " reluctant", " to", " use", " the", " cash", " savings", " to", " fill", " the", " projected", " revenue"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 30, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" void", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "If", " we", " can", " find", " a", " way", " to", " achieve", " greater", " revenue", " stability", " without", " the", " use", " of", " the", " contingency", " account", " \u2014", " though", " it", "\u00e2\u0122", "\u013b", "s", " probably", " not", " possible", " \u2014", " that", " would", " be", " my", " first", " desire", ",", "\u00e2\u0122", "\u013f", " said", " the", " Calgary", " MLA", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Phil", "os", "oph", "ically", ",", " I", " think", " conting", "encies", " are", " there", " to", " smooth", " things", " out", ".", " They", "\u00e2\u0122", "\u013b", "re", " not", " there", " to", " totally", " drain", " the", " bank", " account", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Although", " the", " price", " of", " oil", " has", " stayed", " below", " $", "60", " US", " per", " barrel", " for", " much", " of", " this", " year", ",", " Cec", "i", " said", " he", " hoped", " new", " corporate", " tax", " increases", " and", " hikes", " in", " personal", " income", " tax", " for", " the", " wealthiest", " Albert", "ans", " will", " meet", " some"], "values": [0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 22, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [".", " So", " far", ",", " 1", ",", "160", " bottles", " of", " water", " have", " been", " delivered", " to", " campus", ",", " and", " additional", " distilled", " water", " for", " kitchen", " use", ".", "\n", "\n", "IU", " Health", " Ball", " Memorial", " Hospital", " has", " put", " in", " place", " contingency", " plans", " so", " that", " its", " patients", " and", " employees", " will", " not", " be", " impacted", ".", " Neil", " G", "iff", "ord", ",", " director", " of", " marketing", " and", " communication", ",", " said", " that", " the", " hospital", " has", " placed", " signs", " on", " public", " water", " f", "ount", "ains", ",", " has", " made", " bottled", " water", " available", " to", " patients", " and", " team", " members", " and", " is", " taking", " necessary", " precautions", " during", " food", " preparations", " and", " other", " day", "-", "to", "-", "day", " operations", ".", "\n", "\n", "El", "m", " Street", " Brewing", " Co", ".", " offered", " to", " assist", " those", " in", " need", " of", " water", " today", " at", " their", " business", " on", " 5", "19", " N", " Elm", " St", "."], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 35, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" to", " get", " back", ",", " and", " get", " some", " quality", " practice", " time", " with", " the", " team", " before", " the", " season", " starts", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "D", "ough", "ty", " said", " that", " his", " strength", " and", " conditioning", " has", " improved", " over", " last", " season", ",", " and", " that", " he", " is", " already", " r", "arin", "\u00e2\u0122", "\u013b", " to", " go", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "I", "\u00e2\u0122", "\u013b", "ve", " been", " skating", " back", " home", " pretty", " hard", ",", " so", " I", " think", " I", " can", " jump", " right", " into", " it", ",", "\u00e2\u0122", "\u013f", " said", " Dough", "ty", ".", " \u00e2\u0122", "\u013e", "But", " whenever", " they", " want", " me", " to", " play", ",", " I", "\u00e2\u0122", "\u013b", "m", " available", " to", " play", ",", " and", " I", " want", " to", " get", " back", " in", " action", " as", " soon", " as", " I", " can", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "\u00e2\u0122", "\u013e", "I", " feel", " stronger", ",", " and", " I", " feel", " better", " conditioned"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.547], "qualifying_token_index": 126, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" $", "2", ".", "29", " billion", " US", " if", " all", " the", " options", " and", " purchase", " rights", " are", " exercised", ".", "\n", "\n", "Delivery", " of", " the", " first", " jet", ",", " which", " has", " seating", " for", " 107", " passengers", ",", " is", " expected", " in", " 2016", ".", "\n", "\n", "The", " conditional", " purchase", " agreement", " signed", " on", " Tuesday", " is", " a", " coup", " for", " Bomb", "ard", "ier", ",", " and", " us", "hers", " in", " a", " change", " in", " Canadian", " aviation", ".", " That", "'s", " because", " the", " C", "Series", " jets", " can", " fly", " 5", ",", "400", " km", " without", " ref", "ue", "lling", ",", " much", " farther", " than", " the", " current", " fleet", " of", " Q", "400", " turb", "op", "rop", " planes", " that", " Porter", " flies", " to", " connect", " 19", " cities", " across", " Eastern", " Canada", " and", " the", " U", ".", "S", ".", "\n", "\n", "The", " airline", " said", " the", " expansion", " could", " mean", " 1", ",", "000", " new", " employees", ",", " which", " would"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.351, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 39, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" for", " the", " future", " of", " Porter", " Airlines", " \u2014", " a", " vision", " with", " service", " to", " destinations", " across", " North", " America", ",", " from", " Calgary", " and", " Vancouver", ",", " to", " Los", " Angeles", ",", " Miami", " and", " Orlando", ".\"", "\n", "\n", "The", " move", " pushes", " Porter", " into", " direct", " competition", " with", " Air", " Canada", " and", " West", "Jet", " as", " a", " national", " carrier", ",", " while", " setting", " up", " a", " potential", " political", " standoff", " over", " expansion", " of", " the", " island", " airport", " in", " downtown", " Toronto", ".", "\n", "\n", "The", " conditional", " deal", " is", " to", " buy", " 12", " Bomb", "ard", "ier", " CS", "100", "s", ",", " with", " options", " on", " 18", " more", ".", "\n", "\n", "The", " deal", " also", " includes", " purchase", " rights", " for", " six", " of", " Bomb", "ard", "ier", "'s", " Q", "400", " turb", "op", "rop", " aircraft", ",", " currently", " the", " main", "stay", " of", " the", " Porter", " fleet", ".", "\n", "\n", "The", " total", " purchase", " could", " reach"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 70, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" they", " treated", " me", " with", " a", " confusing", " detachment", " and", " thought", ",", " \"", "Oh", ",", " so", " this", " is", " what", " it", " should", " be", " like", ".\"", "\n", "\n", "\"", "So", ",", " this", " is", " what", " kindness", " is", " like", ".\"", "\n", "\n", "D", "ating", " made", " me", " feel", " like", " the", " l", "onel", "iest", " person", " in", " the", " world", " for", " a", " long", " time", ".", " I", " wish", " now", " I", " hadn", "'t", " done", " it", " at", " all", ",", " but", " withdrawal", " is", " painful", " and", " uncomfortable", ".", " I", " was", " willing", " to", " try", " anything", " to", " feel", " just", " a", " little", " better", ".", "\n", "\n", "You", " will", " miss", " your", " ex", " boyfriend", " in", " a", " way", " you", " didn", "'t", " know", " was", " possible", " and", " you", " don", "'t", " think", " should", " be", " allowed", ".", " You", " will", " want", " to", " get", " back", " together", ".", " Ab", "usive", " relationships", " fuck", " your", " brain", " chemistry"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.336, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 6, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" the", " editor", " or", " write", " to", " letters", "@", "the", "atlantic", ".", "com", ".", "<|endoftext|>", "2015", " brought", " record", " passenger", " traffic", " to", " Bush", ",", " Hobby", "\n", "\n", "A", " Southwest", " Airlines", " Boeing", " 737", " lands", " at", " Hobby", " Airport", " in", " March", " 2015", ".", " A", " Southwest", " Airlines", " Boeing", " 737", " lands", " at", " Hobby", " Airport", " in", " March", " 2015", ".", " Photo", ":", " Bill", " Montgomery", ",", " HC", " Staff", " Photo", ":", " Bill", " Montgomery", ",", " HC", " Staff", " Image", " 1", " of", " /", " 1", " Caption", " Close", " 2015", " brought", " record", " passenger", " traffic", " to", " Bush", ",", " Hobby", " 1", " /", " 1", " Back", " to", " Gallery", "\n", "\n", "Houston", "'s", " two", " major", " airports", " both", " set", " records", " for", " passenger", " traffic", " in", " 2015", ",", " the", " Houston", " Airport", " System", " reported", " Wednesday", ".", "\n", "\n", "H", "obby", " Airport", " had", " 145", ",", "202", " international", " passengers", " between", " opening", " its", " international", " conc", "ourse", " in"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.315, 0.0], "qualifying_token_index": 125, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" upper", " conc", "ourse", " and", " initially", " made", " available", " to", " patrons", " currently", " sitting", " in", " Row", " 1", " of", " the", " 300", " Level", ".", "\n", "\n", "Installation", " of", " new", " seamless", " glass", " in", " the", " front", " row", " of", " the", " 300", " Level", ",", " replacing", " the", " existing", " mesh", " railing", ",", " for", " improved", " sight", "lines", " for", " 300", "-", "level", " patrons", ".", "\n", "\n", "The", " upgrades", " will", " be", " undertaken", " this", " summer", ".", "\n", "\n", "M", "TS", " Centre", ",", " which", " opened", " Nov", ".", " 16", ",", " 2004", ",", " has", " hosted", " almost", " 1", ",", "400", " events", ",", " including", " hockey", " games", ",", " concerts", " and", " other", " entertainment", " events", ".", "<|endoftext|>", "Ad", "orable", " \u2013", " short", " lived", " and", " with", " strained", " relations", ".", "\n", "\n", "Ad", "orable", " \u2013", " in", " concert", " at", " Gl", "aston", "bury", " 1993", " \u2013", " BBC", " Radio", " 1", " \u2013", "\n", "\n", "Hom", "age", " to", " Sho"], "values": [0.0, 0.0, 0.31, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 2, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" the", " United", " States", " and", " worldwide", " marking", " Earth", " Day", " to", " raise", " awareness", " of", " the", " contributions", " of", " science", " to", " society", ",", " and", " the", " importance", " of", " supportive", " public", " policy", ",", " organizers", " said", ".", "\n", "\n", "The", " event", " kicked", " off", " with", " remarks", " from", " area", " scientists", " at", " the", " Community", " Conc", "ourse", " in", " the", " Civic", " Center", " complex", " next", " to", " City", " Hall", ",", " followed", " by", " the", " march", " down", " Broadway", " to", " the", " Water", "front", " Park", " outside", " the", " County", " Administration", " Center", ".", "\n", "\n", "Mar", "chers", " carried", " signs", " saying", " \u00e2\u0122", "\u013e", "build", " science", " not", " walls", ",", "\u00e2\u0122", "\u013f", " \u00e2\u0122", "\u013e", "ice", " has", " no", " agenda", " it", " just", " melts", ",", "\u00e2\u0122", "\u013f", " \u00e2\u0122", "\u013e", "we", " do", " not", " inherit", " the", " Earth", " from", " our", " ancestors", ",", " we", " borrow", " it", " from", " our", " children", ",", "\u00e2\u0122", "\u013f", " \u00e2\u0122", "\u013e", "there", " is", " no"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.306, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 45, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" blame", " everybody", " himself", " for", " everything", " he", " did", " wrong", " will", " stand", " out", " in", " historical", " accounts", " of", " his", " dizz", "ying", " fall", " from", " the", " heights", " vault", "ing", " ambition", ".", "\n", "\n", "If", " you", "\u00e2\u0122", "\u013b", "re", " ready", " to", " read", " more", " from", " the", " unb", "oss", "ed", " and", " unb", "ought", " Polit", "icus", " team", ",", " sign", " up", " for", " our", " newsletter", " here", "!", " Email", " address", ":", " Leave", " this", " field", " empty", " if", " you", "'re", " human", ":", "<|endoftext|>", "In", " the", " conservative", " comment", "ariat", " and", " intellectual", " classes", ",", " the", " deal", " with", " the", " devil", " that", " is", " Trump", "ism", " seems", " increasingly", " complete", ".", " St", "al", "wart", " conservatives", " who", " for", " years", "\u2014", "in", " some", " cases", " decades", "\u2014", "def", "ended", " the", " principles", " of", " limited", " government", ",", " personal", " liberty", ",", " and", " strict", " adherence", " to", " the", " Constitution", " have", " h", "iked", " their", " skirts", " and"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 73, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" third", " and", " final", " presidential", " debate", " on", " Wednesday", " night", ".", " But", " while", " the", " debate", " will", " bring", " the", " eyes", " of", " the", " world", " to", " Las", " Vegas", ",", " it", " has", " brought", " something", " else", " to", " UN", "LV", " basketball", ",", " the", " primary", " tenant", " of", " the", " Thomas", " &", " Mack", " Center", ":", " headaches", ".", " Lots", " and", " lots", " of", " headaches", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "The", " first", " thing", " that", " came", " to", " mind", " was", " \u00e2\u0122", "\u013a", "that", "\u00e2\u0122", "\u013b", "s", " pretty", " cool", ",", "\u00e2\u0122", "\u013b", " the", " national", " attention", " that", " it", " will", " bring", " to", " the", " Thomas", " &", " Mack", " and", " the", " university", "\u00e2\u0122", "\u013f", " new", " UN", "LV", " head", " coach", " Marvin", " Men", "z", "ies", " said", ".", " \u00e2\u0122", "\u013e", "But", " then", " the", " second", " thought", " was", " \u00e2\u0122", "\u013a", "wow", ",", " that", "\u00e2\u0122", "\u013b", "s", " going", " to", " be", " a", " logistical", " nightmare", ".", "\u00e2\u0122"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.188, 0.0, 0.0, 0.0], "qualifying_token_index": 123, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" restoring", " his", " access", ",", " Facebook", " said", " it", " would", " not", " pay", " him", " a", " bounty", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "The", " more", " important", " issue", " here", " is", " with", " how", " the", " bug", " was", " demonstrated", " using", " the", " accounts", " of", " real", " people", " without", " their", " permission", ",", "\u00e2\u0122", "\u013f", " said", " Facebook", " software", " engineer", " Matt", " Jones", " in", " a", " Sunday", " entry", " on", " Hacker", " News", ".", " \u00e2\u0122", "\u013e", "Expl", "o", "iting", " bugs", " to", " impact", " real", " users", " is", " not", " acceptable", " behavior", " for", " a", " white", " hat", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Jones", " did", " acknowledge", " that", " Facebook", " should", " have", " asked", " Sh", "re", "ate", "h", " for", " more", " information", " before", " dismissing", " his", " report", ",", " but", " he", " also", " tick", "ed", " off", " a", " list", " of", " reasons", ",", " including", " the", " fact", " that", " Facebook", " receives", " \u00e2\u0122", "\u013e", "h", "undreds", " of", " reports", " each", " day", "\u00e2\u0122", "\u013f", " and"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.181, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 119, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": ["\u013b", " but", " don", "\u00e2\u0122", "\u013b", "t", " worry", ",", "\u00e2\u0122", "\u013f", " he", " wrote", " in", " a", " series", " of", " tweets", ".", " \u00e2\u0122", "\u013e", "I", " created", " her", " character", " as", " an", " antit", "hesis", " to", " the", " women", " characters", " appeared", " in", " the", " past", " fighting", " game", " who", " are", " excessively", " exposed", ".", " \u00e2\u0122", "\u013a", "Qu", "iet", "\u00e2\u0122", "\u013b", " who", " doesn", "\u00e2\u0122", "\u013b", "t", " have", " a", " word", " will", " be", " teased", " in", " the", " story", " as", " well", ".", " But", " once", " you", " recognise", " the", " secret", " reason", " for", " her", " exposure", ",", " you", " will", " feel", " ashamed", " of", " your", " words", " &", " deeds", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "So", ",", " two", " years", " on", " Phantom", " Pain", " has", " been", " released", ".", " No", " one", " has", " forgotten", " K", "oj", "ima", "\u00e2\u0122", "\u013b", "s", " words", " and", " we", " have", " the", " full", " story", " behind", " Quiet", "\u00e2\u0122", "\u013b", "s", " attire", ".", " Are"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.179, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 26, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [",", " a", " spokesman", " tells", " the", " BBC", ".", " Another", " 800", " will", " be", " there", " by", " Friday", " and", " the", " rest", " of", " the", " 3", ",", "500", "-", "member", " division", " is", " on", " standby", " in", " case", " they", " are", " needed", ".", " The", " US", " 82", "nd", " Air", "borne", " Division", " will", " have", " 100", " members", " in", " Haiti", " on", " Thursday", ",", " a", " spokesman", " tells", " the", " BBC", ".", " Another", " 800", " will", " be", " there", " by", " Friday", " and", " the", " rest", " of", " the", " 3", ",", "500", "-", "member", " division", " is", " on", " standby", " in", " case", " they", " are", " needed", ".", "\n", "\n", "14", "11", " Elizabeth", " Byr", "s", ",", " a", " spokeswoman", " for", " the", " UN", ",", " tells", " the", " BBC", " World", " Service", ":", " \"", "Fire", "men", " have", " also", " been", " trapped", " in", " the", " earthquake", " so", " their", " logistical", " means", " are", " very", " poor", " and", " they", " cannot", " cope", " with", " the", " situation"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.178, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 115, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [".", " That", "'s", " why", " it", "'s", " very", " urgent", " to", " bring", " heavy", " machinery", " to", " remove", " the", " debris", ",", " also", " civil", " and", " military", " assets", " like", " helicopters", " are", " needed", ",", " given", " the", " bad", " conditions", " of", " the", " roads", ".\"", " \"", "Fire", "men", " have", " also", " been", " trapped", " in", " the", " earthquake", " so", " their", " logistical", " means", " are", " very", " poor", " and", " they", " cannot", " cope", " with", " the", " situation", ".", " That", "'s", " why", " it", "'s", " very", " urgent", " to", " bring", " heavy", " machinery", " to", " remove", " the", " debris", ",", " also", " civil", " and", " military", " assets", " like", " helicopters", " are", " needed", ",", " given", " the", " bad", " conditions", " of", " the", " roads", ".\"", "\n", "\n", "14", "00", " Brooke", " D", "urb", "in", ",", " a", " teacher", " in", " Port", "-", "de", "-", "Pa", "ix", " in", " northern", " Haiti", ",", " e", "-", "mails", ":", " \"", "The", " worst", " part", " about", " the", " whole"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.177, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 47, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [".", " Nearly", " a", " quarter", " of", " the", " people", " who", " expressed", " criticism", " of", " K", "omen", "'s", " decision", " online", " said", " they", " were", " going", " to", " pull", " their", " donations", " from", " K", "omen", ".", "\n", "\n", "In", " contrast", ",", " Planned", " Parenthood", " has", " seen", " a", " huge", " influx", " of", " financial", " donations", " in", " the", " 24", " hours", " since", " K", "omen", " broke", " ties", ".", " While", " the", " organization", " has", " not", " officially", " released", " the", " new", " donation", " numbers", " yet", ",", " a", " source", " close", " to", " the", " issue", " said", " they", "'ve", " raised", " \"", "h", "undreds", " of", " thousands", "\"", " of", " dollars", " in", " individual", " donations", " during", " that", " period", ".", " That", ",", " combined", " with", " a", " donation", " of", " $", "250", ",", "000", " from", " Texas", " oil", " executive", " Lee", " F", "ikes", " and", " his", " wife", " Amy", " for", " a", " \"", "Bre", "ast", " Health", " Emergency", " Fund", ",\"", " could", " put", " the", " family", " planning"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.173, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 78, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": ["\u013b", "\u00e2\u0122", "\u013f", "\n", "\n", "Ad", "mittedly", ",", " the", " debate", " has", " been", " a", " logistical", " nightmare", " for", " just", " about", " everyone", " on", " UN", "LV", "\u00e2\u0122", "\u013b", "s", " campus", ",", " and", " very", " likely", " for", " the", " entire", " city", " of", " Las", " Vegas", " in", " recent", " days", ".", " However", ",", " it", "\u00e2\u0122", "\u013b", "s", " taken", " an", " especially", " large", " toll", " on", " the", " UN", "LV", " basketball", " team", " because", " its", " offices", " are", " located", " in", " the", " Thomas", " &", " Mack", " and", " its", " practice", " facility", " is", " located", " right", " next", " door", " at", " the", " Mend", "enh", "all", " Center", ".", "\n", "\n", "To", " prepare", " for", " the", " debate", " and", " to", " make", " sure", " the", " entire", " surrounding", " area", " is", " completely", " secure", ",", " the", " Run", "nin", "\u00e2\u0122", "\u013b", " Rebels", " hoops", " team", " was", " forced", " to", " leave", " its", " offices", " last", " Friday", " and", " won", "'t", " be", " able", " to", " return", " until"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.169, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 13, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" 3", ".", "2", " million", " \u2014", " more", " than", " a", " third", " of", " Somalia", "\u00e2\u0122", "\u013b", "s", " population", ".", "\n", "\n", "Canadian", " naval", " ships", " are", " esc", "orting", " World", " Food", " Program", " shipments", " to", " Mog", "adish", "u", " until", " September", ",", " and", " U", ".", "N", ".", " officials", " say", " it", " is", " hoped", " that", " French", " and", " then", " German", " forces", " will", " take", " over", ".", "\n", "\n", "Further", " north", " in", " the", " Gulf", " of", " Aden", ",", " the", " recent", " attacks", " have", " also", " st", "ung", " the", " anti", "-", "terrorist", " Combined", " Task", " Force", " 150", " into", " action", ".", " The", " multinational", " unit", ",", " part", " of", " Washington", "\u00e2\u0122", "\u013b", "s", " Operation", " End", "uring", " Freedom", ",", " is", " based", " in", " neighboring", " Dj", "ib", "out", "i", " and", " has", " come", " to", " the", " aid", " of", " many", " ships", " attacked", " by", " pirates", ".", "\n", "\n", "This", " week", ",", " it", " announced", " a"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.166, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 84, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" speaking", " youth", " as", " un", "qualified", ",", " when", " the", " fact", " is", " it", " has", " nothing", " to", " do", " with", " the", " actual", " skills", " required", ".", "R", "aj", "esh", " Sharma", " joined", " Pat", "an", "j", "ali", " as", " senior", " manager", " of", " sales", " and", " marketing", " a", " month", " ago", " after", " 14", " years", " in", " companies", " such", " as", " Rev", "lon", " and", " J", "L", " Morrison", ",", " having", " been", " struck", " by", " a", " Discovery", " Channel", " report", " on", " the", " big", " profits", " that", " multinational", " F", "MC", "G", " companies", " made", " in", " India", ".", "\u00e2\u0122", "\u013e", "When", " I", " heard", " about", " Pat", "an", "j", "ali", ",", " I", " was", " willing", " to", " even", " take", " a", " pay", " cut", " to", " join", ",", "\u00e2\u0122", "\u013f", " he", " said", ".", " \u00e2\u0122", "\u013e", "Pat", "an", "j", "ali", " not", " only", " matched", " my", " compensation", " expectations", " but", " also", " offered", " a", " work", " culture", " rich", " in", " diversity", " of", " talent"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.164, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 67, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": -1, "bin_max": 3.429, "bin_contains": -1, "tokens": [" the", " principal", " to", " make", " sure", " the", " gaming", " console", " would", " be", " well", " used", ",", " Nintendo", " sent", " a", " DS", " and", " a", " \"", "game", " suitable", " for", " children", " to", " play", "\"", " to", " the", " school", ".", "\n", "\n", "Ms", " Murphy", " said", " it", " wasn", "'t", " common", " practice", " to", " give", " away", " free", " consoles", " \u2014", " or", " even", " reply", " \u2014", " to", " everyone", " who", " wrote", " to", " the", " company", ".", " She", " said", " it", " received", " \"", "h", "undreds", "\"", " of", " letters", " per", " day", ".", "\n", "\n", "\"", "Nintendo", " does", " not", " routinely", " give", " away", " products", " in", " response", " to", " letters", ".", " From", " time", " to", " time", ",", " we", " may", " donate", " product", " to", " schools", ",", " hospitals", " or", " retirement", " villages", ",\"", " she", " said", ".", "\n", "\n", "\"", "We", " receive", " a", " high", " number", " of", " letters", " on", " a", " daily", " basis", ",", " and", " unfortunately", " we", " are", " not"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 65, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 2.743, "bin_max": 3.429, "bin_contains": 1e-05, "tokens": ["about", " global", " warming", ",", " or", " the", " historical", " Jesus", ",", " or", " the", " way", " we", " raise", " our", " children", ".", " That", "\u00e2\u0122", "\u013b", "s", " why", " they", " need", " to", " have", " special", " protections", ".", "\n", "\n", "*", " *", " *", "\n", "\n", "But", " the", " tenure", " system", ",", " which", " is", " already", " being", " eroded", " by", " the", " growth", " of", " contingent", " labor", ",", " is", " not", " the", " only", " thing", " that", " is", " under", " assault", " in", " the", " top", "-", "down", ",", " corpor", "at", "ized", " academy", ".", " As", " Cary", " Nelson", " explains", " in", " No", " University", " Is", " an", " Island", " (", "2010", "),", " shared", " governance", "\u2014", "the", " principle", " that", " universities", " should", " be", " controlled", " by", " their", " faculties", ",", " which", " protects", " academic", " values", " against", " the", " enc", "roach", "ments", " of", " the", " spreadsheet", " brigade", "\u2014", "is", " also", " threatened", " by", " the", " changing", " structure", " of", " academic", " work", ".", " Cont", "ing"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.415, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 50, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 2.743, "bin_max": 3.429, "bin_contains": 1e-05, "tokens": [".", "7", ".", "2014", ")", " with", " two", " dozen", " top", " managers", " in", " tow", ",", " including", " five", " heads", " of", " DA", "X", "-", "listed", " companies", ".", " Since", " Tuesday", ",", " the", " final", " day", " of", " Merkel", "'s", " China", " visit", ",", " it", "'s", " been", " evident", " that", " the", " large", " German", " business", " contingent", " made", " less", " of", " an", " impression", " on", " the", " Chinese", " than", " some", " of", " the", " members", " of", " the", " delegation", " had", " hoped", ".", "\n", "\n", "In", " the", " end", ",", " the", " Chinese", " were", " not", " quite", " as", " open", " to", " everything", " the", " German", " business", " leaders", " had", " on", " their", " wish", " list", " as", " the", " Germans", " would", " have", " liked", ".", " Acc", "ords", " amount", "ing", " to", " 3", " billion", " euros", " ($", "4", " billion", ")", " were", " signed", ",", " which", " is", " not", " bad", " at", " all", ".", " However", ",", " it", "'s", " a", " relatively", " small", " package", " if", " you"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.411, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.109, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 44, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 2.743, "bin_max": 3.429, "bin_contains": 1e-05, "tokens": ["el", "tering", " gym", " bed", "e", "ck", "ed", " with", " championship", " banners", ",", " the", " governor", " is", " received", " by", " a", " bo", "ister", "ous", " contingent", " of", " his", " old", " friends", " from", " the", " class", " of", " 1980", ".", " \u00e2\u0122", "\u013e", "Lots", " of", " people", " have", " asked", " me", " over", " the", " course", " of", " last", " week", ",", " why", " here", "?", "\u00e2\u0122", "\u013f", " he", " says", ".", " \u00e2\u0122", "\u013e", "Why", " here", "?", " Because", " everything", " started", " here", " for", " me", ".", " The", " confidence", ".", " The", " education", ".", " The", " friends", ".", " The", " family", ".", " And", " the", " love", " that", " I", "\u00e2\u0122", "\u013b", "ve", " always", " felt", " for", " and", " from", " this", " community", ".", "\u00e2\u0122", "\u013f", " Outside", " the", " gym", ",", " protesters", " pick", "et", " the", " speech", ",", " waving", " signs", " that", " read", " B", "ULL", "Y", ".", "\n", "\n", "On", " the", " campaign", " trail", ",", " he", " keeps", " getting", " incred", "ulous", " questions"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 20, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 2.743, "bin_max": 3.429, "bin_contains": 1e-05, "tokens": [" if", " we", "\u00e2\u0122", "\u013b", "re", " going", " to", " reach", " the", " oft", "-", "repe", "ated", " goal", " of", " universal", " post", "secondary", " education", ",", " an", " objective", " that", " would", " double", " enroll", "ments", "\u2014", "we", "\u00e2\u0122", "\u013b", "re", " going", " to", " need", " a", " lot", " more", " teachers", ":", " well", " paid", ",", " institution", "ally", " supported", ",", " socially", " valued", ".", " As", " of", " 2003", " there", " were", " about", " 400", ",", "000", " tenure", "-", "track", " professors", " in", " the", " United", " States", " (", "as", " compared", " with", " about", " 6", " million", " primary", "-", " and", " secondary", "-", "school", " teachers", ").", " Between", " reducing", " class", " sizes", ",", " reversing", " the", " shift", " to", " contingent", " labor", " and", " beef", "ing", " up", " our", " college", "-", "com", "pletion", " rates", ",", " we", "\u00e2\u0122", "\u013b", "re", " going", " to", " need", " at", " least", " five", " times", " as", " many", ".", "\n", "\n", "So", " where", "\u00e2\u0122", "\u013b", "s", " the", " money"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 91, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 2.743, "bin_max": 3.429, "bin_contains": 1e-05, "tokens": [" the", " strategic", " channel", " at", " high", " risk", " of", " \u00e2\u0122", "\u013e", "war", ",", " strikes", ",", " terrorism", " and", " related", " per", "ils", "\u00e2\u0122", "\u013f", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "But", " it", "\u00e2\u0122", "\u013b", "s", " just", " a", " recommendation", ",", " and", " some", " under", "writers", " may", " not", " follow", " it", " for", " their", " very", " important", " clients", ",", "\u00e2\u0122", "\u013f", " Muk", "und", "an", " told", " Reuters", ".", " \u00e2\u0122", "\u013e", "Cost", "s", " have", " not", " gone", " up", ".", " Of", " course", ",", " if", " you", " are", " hijacked", " they", " go", " up", " quite", " significantly", ".", " But", " there", " is", " no", " contingent", " cost", " to", " piracy", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "S", "om", "ali", " pirates", " are", " currently", " holding", " about", " 130", " crew", " members", " hostage", " on", " at", " least", " seven", " vessels", ",", " including", " huge", " chemical", " tank", "ers", " and", " bulk", "-", "car", "riers", ".", " Gun", "men", " are", " holding", " vessels", " from", " Japan"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 3.429, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 82, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.686, "bin_max": 1.372, "bin_contains": 1e-05, "tokens": [" red", ".", "\n", "\n", "The", " 2014", "-", "15", " consolidated", " surplus", " of", " $", "1", ".", "1", " billion", " ends", " of", " string", " of", " six", " consecutive", " deficits", " and", " boosts", " the", " province", "\u00e2\u0122", "\u013b", "s", " contingency", " fund", " to", " $", "8", ".", "2", " billion", " \u2014", " a", " reserve", " Finance", " Minister", " Joe", " Cec", "i", " says", " will", " come", " in", " handy", " in", " dealing", " with", " the", " drop", " in", " non", "-", "ren", "ew", "able", " resource", " revenue", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Going", " forward", " the", " climate", " \u2014", " the", " economy", " today", " \u2014", " is", " very", " different", " from", " what", " it", " was", " then", ",", "\u00e2\u0122", "\u013f", " he", " told", " reporters", " at", " the", " legislature", " Tuesday", " as", " the", " NDP", " government", " released", " the", " final", " year", "-", "end", " numbers", ".", "\n", "\n", "But", " Cec", "i", " said", " he", " was", " reluctant", " to", " use", " the", " cash", " savings", " to", " fill", " the", " projected", " revenue"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.03, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 30, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.686, "bin_max": 1.372, "bin_contains": 1e-05, "tokens": [" void", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "If", " we", " can", " find", " a", " way", " to", " achieve", " greater", " revenue", " stability", " without", " the", " use", " of", " the", " contingency", " account", " \u2014", " though", " it", "\u00e2\u0122", "\u013b", "s", " probably", " not", " possible", " \u2014", " that", " would", " be", " my", " first", " desire", ",", "\u00e2\u0122", "\u013f", " said", " the", " Calgary", " MLA", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Phil", "os", "oph", "ically", ",", " I", " think", " conting", "encies", " are", " there", " to", " smooth", " things", " out", ".", " They", "\u00e2\u0122", "\u013b", "re", " not", " there", " to", " totally", " drain", " the", " bank", " account", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Although", " the", " price", " of", " oil", " has", " stayed", " below", " $", "60", " US", " per", " barrel", " for", " much", " of", " this", " year", ",", " Cec", "i", " said", " he", " hoped", " new", " corporate", " tax", " increases", " and", " hikes", " in", " personal", " income", " tax", " for", " the", " wealthiest", " Albert", "ans", " will", " meet", " some"], "values": [0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 22, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.686, "bin_max": 1.372, "bin_contains": 1e-05, "tokens": [" void", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "If", " we", " can", " find", " a", " way", " to", " achieve", " greater", " revenue", " stability", " without", " the", " use", " of", " the", " contingency", " account", " \u2014", " though", " it", "\u00e2\u0122", "\u013b", "s", " probably", " not", " possible", " \u2014", " that", " would", " be", " my", " first", " desire", ",", "\u00e2\u0122", "\u013f", " said", " the", " Calgary", " MLA", ".", "\n", "\n", "\u00e2\u0122", "\u013e", "Phil", "os", "oph", "ically", ",", " I", " think", " conting", "encies", " are", " there", " to", " smooth", " things", " out", ".", " They", "\u00e2\u0122", "\u013b", "re", " not", " there", " to", " totally", " drain", " the", " bank", " account", ".", "\u00e2\u0122", "\u013f", "\n", "\n", "Although", " the", " price", " of", " oil", " has", " stayed", " below", " $", "60", " US", " per", " barrel", " for", " much", " of", " this", " year", ",", " Cec", "i", " said", " he", " hoped", " new", " corporate", " tax", " increases", " and", " hikes", " in", " personal", " income", " tax", " for", " the", " wealthiest", " Albert", "ans", " will", " meet", " some"], "values": [0.008, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 59, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.686, "bin_max": 1.372, "bin_contains": 1e-05, "tokens": [".", " So", " far", ",", " 1", ",", "160", " bottles", " of", " water", " have", " been", " delivered", " to", " campus", ",", " and", " additional", " distilled", " water", " for", " kitchen", " use", ".", "\n", "\n", "IU", " Health", " Ball", " Memorial", " Hospital", " has", " put", " in", " place", " contingency", " plans", " so", " that", " its", " patients", " and", " employees", " will", " not", " be", " impacted", ".", " Neil", " G", "iff", "ord", ",", " director", " of", " marketing", " and", " communication", ",", " said", " that", " the", " hospital", " has", " placed", " signs", " on", " public", " water", " f", "ount", "ains", ",", " has", " made", " bottled", " water", " available", " to", " patients", " and", " team", " members", " and", " is", " taking", " necessary", " precautions", " during", " food", " preparations", " and", " other", " day", "-", "to", "-", "day", " operations", ".", "\n", "\n", "El", "m", " Street", " Brewing", " Co", ".", " offered", " to", " assist", " those", " in", " need", " of", " water", " today", " at", " their", " business", " on", " 5", "19", " N", " Elm", " St", "."], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.028, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 35, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.686, "bin_contains": 0.9999800000000001, "tokens": [",", " which", " deep", "ened", " the", " communal", " divide", " at", " a", " time", " when", " a", " united", " front", " against", " colonial", " rule", " was", " needed", ".", " Post", " independence", ",", " Sav", "ark", "ar", " was", " also", " implicated", " in", " Mah", "at", "ma", " Gandhi", "\u00e2\u0122", "\u013b", "s", " murder", ".", "\n", "\n", "Such", " is", " the", " man", " who", " was", " declared", " by", " Prime", " Minister", " Narendra", " Modi", " to", " be", " \u00e2\u0122", "\u013e", "the", " true", " son", " of", " Mother", " India", " and", " inspiration", " for", " many", " people", "\u00e2\u0122", "\u013f", ",", " in", " his", " Twitter", " sal", "utation", " to", " Sav", "ark", "ar", " on", " his", " birth", " anniversary", " on", " May", " 28", " last", " year", ".", " In", " 2015", ",", " commemor", "ating", " Sav", "ark", "ar", " on", " his", " 132", "nd", " birth", " anniversary", ",", " the", " prime", " minister", " bowed", " before", " a", " portrait", " of", " the", " Hind", "ut", "va", " icon", " in", " remembrance", " of", " \u00e2\u0122", "\u013e", "his", " ind", "om", "itable"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 107, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.686, "bin_contains": 0.9999800000000001, "tokens": [",", " the", " story", " suggests", " ways", " that", " comics", " can", " be", " pushed", " in", " new", " directions", ",", " and", " how", " you", " can", " use", " pictures", " and", " words", ".", " That", "\u00e2\u0122", "\u013b", "s", " basically", " a", " wide", "-", "open", " art", " form", ".", " You", " can", " do", " anything", ".", " Put", " some", " pictures", " in", " any", " combination", ".", " That", "\u00e2\u0122", "\u013b", "s", " the", " part", " that", "\u00e2\u0122", "\u013b", "s", " exciting", " to", " me", "\u2014", "how", " limitless", " the", " possibilities", " are", ",", " as", " you", " sit", " at", " your", " table", " and", " think", " all", " day", ".", "\n", "\n", "P", "aste", ":", " The", " more", " read", " I", " read", " your", " comics", ",", " the", " more", " I", " realize", " how", " important", " a", " sense", " of", " place", " is", " to", " your", " narratives", ".", " It", "\u00e2\u0122", "\u013b", "s", " often", " one", " of", " the", " most", " important", " characters", " in", " your", " books", ",", " thinking", " back", " to", " the", " myriad", " international"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 111, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.686, "bin_contains": 0.9999800000000001, "tokens": [" misguided", ".", "\n", "\n", "Fl", "av", "io", " Vol", "pe", " and", " j", "erry", " D", "ias", ",", " pictured", " here", ",", " say", " critics", " of", " the", " government", "\u00e2\u0122", "\u013b", "s", " decision", " on", " the", " TPP", " are", " misguided", ".", " (", " Cole", " Bur", "ston", " /", " Toronto", " Star", " )", "\n", "\n", "The", " presidents", " of", " the", " Autom", "otive", " Parts", " Manufact", "urers", " Association", " and", " Un", "if", "or", ",", " Canada", "\u00e2\u0122", "\u013b", "s", " largest", " private", " sector", " union", " which", " represents", " aut", "ow", "ork", "ers", ",", " say", " that", "\u00e2\u0122", "\u013b", "s", " because", " the", " TPP", " is", " bad", " for", " the", " small", " and", " medium", "-", "sized", " auto", " parts", " companies", " and", " their", " employees", ".", " Read", " more", ":", " Canada", "\u00e2\u0122", "\u013b", "s", " decision", " to", " decline", " TPP", " agreement", " shouldn", "\u00e2\u0122", "\u013b", "t", " have", " been", " a", " surprise", ":", " Trudeau", "\n", "\n", "Article", " Continued", " Below", "\n", "\n", "Thomas"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 118, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.686, "bin_contains": 0.9999800000000001, "tokens": [" The", " loot", " tables", " will", " be", " getting", " an", " overhaul", " as", " well", " once", " we", " begin", " to", " implement", " our", " unique", " recipes", " and", " rare", " items", " to", " the", " game", ".", "\n", "\n", "N", "amed", " Unique", " Rare", " M", "obs", "\n", "\n", "We", " also", " will", " be", " implementing", " the", " mechanics", " for", " our", " new", " Named", " Unique", " Rare", " M", "obs", ".", " Although", " we", " may", " only", " get", " a", " few", " in", " with", " the", " next", " patch", ",", " our", " upcoming", " community", " competition", " will", " provide", " you", " with", " the", " opportunity", " to", " create", " some", " of", " the", " additions", " we", " will", " add", " throughout", " Early", " Access", ".", " Once", " the", " mechanics", " go", " into", " the", " game", ",", " it", " becomes", " much", " easier", " for", " us", " to", " add", " to", " this", " system", " and", " continuously", " implement", " new", " unique", " mobs", ".", "\n", "\n", "These", " mobs", " will", " be", " named", ",", " have", " stronger", " attributes", " as", " well", " as"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 98, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}, {"bin_min": 0.0, "bin_max": 0.686, "bin_contains": 0.9999800000000001, "tokens": [" Y", "P", " drafted", " a", " totally", " grat", "is", " Swamp", "ie", " for", " James", " to", " treasure", " for", " ever", ".", " It", " was", " all", " incredibly", " sweet", ".", "\n", "\n", "I", " then", " proceeded", " to", " walk", " away", " from", " Yan", "ick", "\u00e2\u0122", "\u013b", "s", " stall", " without", " paying", " for", " ANY", " of", " the", " stuff", " I", " had", " taken", "!", " Like", " a", " th", "ieving", " To", "e", " Rag", "**", ".", "\n", "\n", "Contact", "ing", " him", " on", " Twitter", " the", " same", " day", " to", " apologise", " this", " was", " the", " response", " I", " got", ":", "\n", "\n", "I", "\u00e2\u0122", "\u013b", "d", " like", " to", " nominate", " Yan", "ick", " Pa", "qu", "ette", " for", " the", " \u00e2\u0122", "\u013a", "Nic", "est", " Artist", " at", " LS", "CC", " Award", "\u00e2\u0122", "\u013b", "***", "\n", "\n", "*", "watching", " net", "flix", "\n", "\n", "**", " To", "e", " Rag", " noun", ",", " British", ",", " Inform", "al", " /", "t", "\u00c9", "\u013b", "\u00ca"], "values": [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], "qualifying_token_index": 9, "dfa_values": [], "dfa_maxValue": 0, "dfa_targetIndex": -1}]}]}